
# Get token for Graph (change scopes if needed)
token=$(az account get-access-token --resource-type ms-graph --query accessToken -o tsv)

# Call Graph API
curl -H "Authorization: Bearer $token" \
     -H "Content-Type: application/json" \
     "https://graph.microsoft.com/v1.0/servicePrincipals?\$filter=appId eq 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'&\$select=displayName,id,appId"


curl -H "Authorization: Bearer $token" \
     "https://graph.microsoft.com/v1.0/servicePrincipals/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx?\$select=displayName"




$env:Path -split ';' | Where-Object { $_ -like '*Azure CLI*' }



Invoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\AzureCLI.msi; Start-Process msiexec.exe -Wait -ArgumentList '/I AzureCLI.msi /quiet'



-----------

from pyspark.sql.types import (
    StructType, StructField, StringType, LongType, DoubleType
)
from pyspark.sql.functions import col

# Your catalog choice (use specific one if possible to reduce noise)
# catalog_name = "your_bronze_catalog"  # ← uncomment and set

df_tables = spark.sql("""
  SELECT 
    table_catalog,
    table_schema,
    table_name,
    table_type,
    CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_name
  FROM system.information_schema.tables
  WHERE table_type IN ('MANAGED', 'EXTERNAL')          -- exclude VIEW, MATERIALIZED VIEW, etc.
    AND is_temporary = false
  ORDER BY table_catalog, table_schema, table_name
""")

# If targeting one catalog only (safer/faster):
# df_tables = spark.sql(f"""
#   SELECT table_catalog, table_schema, table_name, table_type,
#          CONCAT('{catalog_name}', '.', table_schema, '.', table_name) AS full_name
#   FROM {catalog_name}.information_schema.tables
#   WHERE table_type IN ('MANAGED', 'EXTERNAL') AND is_temporary = false
# """)

tables_list = [row.full_name for row in df_tables.collect()]
print(f"Found {len(tables_list)} eligible tables (managed/external only).")

# Explicit schema for final DF (prevents empty inference issues)
result_schema = StructType([
    StructField("catalog",    StringType(), True),
    StructField("schema",     StringType(), True),
    StructField("table",      StringType(), True),
    StructField("full_name",  StringType(), True),
    StructField("table_type", StringType(), True),
    StructField("format",     StringType(), True),
    StructField("num_files",  LongType(),   True),
    StructField("size_bytes", LongType(),   True),
    StructField("size_gb",    DoubleType(), True),
    StructField("location",   StringType(), True),
    StructField("error",      StringType(), True)
])

results = []

for full_table in tables_list:
    try:
        detail_df = spark.sql(f"DESCRIBE DETAIL `{full_table}`")
        if detail_df.count() == 0:
            raise Exception("DESCRIBE DETAIL returned no rows")
        
        row = detail_df.collect()[0]
        
        results.append({
            "catalog":    row.get("catalog", full_table.split(".")[0]),
            "schema":     row.get("schema", full_table.split(".")[1]),
            "table":      row.get("name", full_table.split(".")[2]),
            "full_name":  full_table,
            "table_type": row.get("type", row.get("tableType", "TABLE")),
            "format":     row.get("format", "DELTA"),  # assume Delta if missing
            "num_files":  row.get("numFiles", 0) or 0,
            "size_bytes": row.get("sizeInBytes", 0) or 0,
            "size_gb":    round((row.get("sizeInBytes", 0) or 0) / (1024**3), 2),
            "location":   row.get("location"),
            "error":      None
        })
    except Exception as e:
        err_str = str(e)
        if "TABLE_OR_VIEW_NOT_FOUND" in err_str or "not found" in err_str.lower():
            results.append({
                "full_name": full_table,
                "error": "TABLE_OR_VIEW_NOT_FOUND - likely a view, dropped table, or permission issue"
            })
        else:
            results.append({
                "full_name": full_table,
                "error": err_str[:400]
            })

# Create DF safely
result_df = spark.createDataFrame(results, schema=result_schema) if results else spark.createDataFrame([], result_schema)

# Display
result_df.filter(col("error").isNull()) \
  .orderBy(col("size_bytes").desc_nulls_last()) \
  .select("full_name", "size_gb", "num_files", "format", "location") \
  .show(20, truncate=False)

print("Summary of failures:")
result_df.filter(col("error").isNotNull()).select("full_name", "error").show(truncate=False)

# Totals (only successful ones)
result_df.filter(col("error").isNull()).agg(
    sum("size_gb").alias("total_size_gb"),
    sum("num_files").alias("total_files"),
    count("*").alias("successful_tables")
).show()





---------------- 



from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, LongType, DoubleType, 
    IntegerType
)
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

# ──────────────────────────────────────────────
# 1. Get list of all tables in the catalog(s)
# ──────────────────────────────────────────────
# Option A: All accessible catalogs (via system schema)
df_tables = spark.sql("""
  SELECT 
    table_catalog,
    table_schema,
    table_name,
    table_type,
    CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_name
  FROM system.information_schema.tables
  WHERE table_type IN ('MANAGED', 'EXTERNAL')
    AND is_temporary = false
  ORDER BY table_catalog, table_schema, table_name
""")

# Option B: Specific catalog only (uncomment & change name if preferred)
# catalog_name = "main"
# df_tables = spark.sql(f"""
#   SELECT 
#     '{catalog_name}' AS table_catalog,
#     table_schema,
#     table_name,
#     table_type,
#     CONCAT('{catalog_name}', '.', table_schema, '.', table_name) AS full_name
#   FROM {catalog_name}.information_schema.tables
#   WHERE table_type IN ('MANAGED', 'EXTERNAL')
#     AND is_temporary = false
# """)

# Collect full table names
tables_list = [row.full_name for row in df_tables.select("full_name").collect()]

print(f"Found {len(tables_list)} tables to process.")

# ──────────────────────────────────────────────
# 2. Define the expected schema for the final result (prevents empty inference error)
# ──────────────────────────────────────────────
result_schema = StructType([
    StructField("catalog",    StringType(),  True),
    StructField("schema",     StringType(),  True),
    StructField("table",      StringType(),  True),
    StructField("full_name",  StringType(),  True),
    StructField("table_type", StringType(),  True),
    StructField("format",     StringType(),  True),
    StructField("num_files",  LongType(),    True),
    StructField("size_bytes", LongType(),    True),
    StructField("size_gb",    DoubleType(),  True),
    StructField("location",   StringType(),  True),
    StructField("error",      StringType(),  True)   # optional error message
])

# ──────────────────────────────────────────────
# 3. Loop and gather stats
# ──────────────────────────────────────────────
results = []

for full_table in tables_list:
    try:
        detail_df = spark.sql(f"DESCRIBE DETAIL `{full_table}`")
        
        # If DESCRIBE DETAIL returns 0 rows → rare but possible (e.g. view-like, broken table)
        if detail_df.count() == 0:
            raise Exception("DESCRIBE DETAIL returned empty result")
        
        row = detail_df.collect()[0]
        
        # Safely extract fields (some columns may vary or be null)
        database_part = row["database"] if "database" in row.asDict() else ""
        schema_name = row.get("schema", database_part.split(".")[-1] if "." in database_part else database_part)
        table_name  = row.get("name", full_table.split(".")[-1])
        
        results.append({
            "catalog":    full_table.split(".")[0],
            "schema":     schema_name,
            "table":      table_name,
            "full_name":  full_table,
            "table_type": row.get("type", row.get("tableType", "UNKNOWN")),
            "format":     row.get("format", "UNKNOWN"),
            "num_files":  row.get("numFiles", 0) or 0,
            "size_bytes": row.get("sizeInBytes", 0) or 0,
            "size_gb":    round((row.get("sizeInBytes", 0) or 0) / (1024 ** 3), 2),
            "location":   row.get("location", None),
            "error":      None
        })
        
    except Exception as e:
        # Capture failures (permission, table dropped, non-Delta, etc.)
        results.append({
            "catalog":    full_table.split(".")[0] if "." in full_table else None,
            "schema":     None,
            "table":      None,
            "full_name":  full_table,
            "table_type": None,
            "format":     None,
            "num_files":  None,
            "size_bytes": None,
            "size_gb":    None,
            "location":   None,
            "error":      str(e)[:500]  # truncate long errors
        })

# ──────────────────────────────────────────────
# 4. Create final DataFrame — now safe even if results is empty
# ──────────────────────────────────────────────
if results:
    result_df = spark.createDataFrame(results, schema=result_schema)
else:
    # Edge case: literally zero tables processed → still create empty DF with schema
    result_df = spark.createDataFrame([], schema=result_schema)

# ──────────────────────────────────────────────
# 5. Show results
# ──────────────────────────────────────────────
print("Top 20 largest tables:")
result_df.select(
    "catalog", "schema", "table", "full_name",
    "format", "num_files", "size_gb", "size_bytes", "error"
).orderBy(col("size_bytes").desc_nulls_last()).show(20, truncate=False)

print("Grand totals:")
result_df.agg(
    {"num_files":   "sum"},
    {"size_bytes":  "sum"},
    {"size_gb":     "sum"}
).withColumnRenamed("sum(num_files)",   "total_files") \
 .withColumnRenamed("sum(size_bytes)",  "total_size_bytes") \
 .withColumnRenamed("sum(size_gb)",     "total_size_gb") \
 .show(truncate=False)

# Optional: save for later use
# result_df.write.mode("overwrite").saveAsTable("your_catalog.your_schema.table_sizes_report")




---


# PySpark in notebook – adapt catalog name
catalog = "bronze_catalog"  # ← change this

df_tables = spark.sql(f"""
  SELECT 
    table_catalog,
    table_schema,
    table_name,
    CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_name
  FROM {catalog}.information_schema.tables
  WHERE table_type IN ('MANAGED', 'EXTERNAL')
    AND is_temporary = FALSE
""")

tables = [r.full_name for r in df_tables.collect()]

results = []
for t in tables:
  try:
    detail = spark.sql(f"DESCRIBE DETAIL `{t}`").collect()[0]
    results.append({
      "full_name": t,
      "num_files": detail.numFiles or 0,
      "size_gb": round((detail.sizeInBytes or 0) / (1024**3), 2)
    })
  except:
    pass  # skip broken tables

result_df = spark.createDataFrame(results)
result_df.orderBy("size_gb", ascending=False).show(50, truncate=False)

result_df.agg(
  {"size_gb": "sum"}
).withColumnRenamed("sum(size_gb)", "total_bronze_gb").show()




----------------

from pyspark.sql.functions import col, concat_ws, lit

# ──────────────────────────────────────────────
# Option 1: All catalogs you have access to
# ──────────────────────────────────────────────
df_tables = spark.sql("""
  SELECT 
    table_catalog,
    table_schema,
    table_name,
    table_type,
    CONCAT(table_catalog, '.', table_schema, '.', table_name) AS full_name
  FROM system.information_schema.tables
  WHERE table_type IN ('MANAGED', 'EXTERNAL')    -- usually what you want
    AND is_temporary = false
  ORDER BY table_catalog, table_schema, table_name
""")

# ──────────────────────────────────────────────
# Option 2: Only one specific catalog (recommended if you know it)
# ──────────────────────────────────────────────
# catalog_name = "main"          # ← change here
# df_tables = spark.sql(f"""
#   SELECT 
#     '{catalog_name}' AS table_catalog,
#     table_schema,
#     table_name,
#     table_type,
#     CONCAT('{catalog_name}', '.', table_schema, '.', table_name) AS full_name
#   FROM {catalog_name}.information_schema.tables
#   WHERE table_type IN ('MANAGED', 'EXTERNAL')
#     AND is_temporary = false
# """)

# Collect list (small enough for most orgs)
tables_list = [row.full_name for row in df_tables.select("full_name").collect()]

# Prepare result list
results = []

for full_table in tables_list:
  try:
    detail_df = spark.sql(f"DESCRIBE DETAIL `{full_table}`")
    row = detail_df.collect()[0]
    
    results.append({
      "catalog": row["database"].split(".")[0] if "." in row["database"] else row["database"],  # sometimes format varies
      "schema": row["schema"] if "schema" in row else row["database"].split(".")[-1],
      "table": row["name"].split(".")[-1],
      "full_name": full_table,
      "table_type": row["type"] if "type" in row else row["format"],
      "format": row["format"],
      "num_files": row["numFiles"] if row["numFiles"] is not None else 0,
      "size_bytes": row["sizeInBytes"] if row["sizeInBytes"] is not None else 0,
      "size_gb": round(row["sizeInBytes"] / (1024**3), 2) if row["sizeInBytes"] else 0.0,
      "location": row["location"]
    })
  except Exception as e:
    # Some tables may fail (deleted, no permission, external non-delta, etc.)
    results.append({
      "full_name": full_table,
      "size_bytes": None,
      "size_gb": None,
      "error": str(e)
    })

# Create final DataFrame
result_df = spark.createDataFrame(results)

# Show summary
result_df.select(
  "catalog", "schema", "table", "full_name",
  "format", "num_files", "size_gb", "size_bytes"
).orderBy(col("size_bytes").desc()).show(100, truncate=False)

# Grand totals
result_df.agg(
  count("*").alias("total_tables"),
  sum("size_bytes").alias("total_size_bytes"),
  (sum("size_bytes") / (1024**3)).alias("total_size_gb")
).show(truncate=False)

# Optionally save
# result_df.write.mode("overwrite").saveAsTable("my_catalog.my_schema.table_sizes")



--------------

import java.sql.*;
public class TestJDBC {
    public static void main(String[] args) {
        try {
            Class.forName("com.databricks.client.jdbc.Driver");
            Connection conn = DriverManager.getConnection("<your-full-jdbc-url>");
            System.out.println("Connected!");
            conn.close();
        } catch (Exception e) { e.printStackTrace(); }
    }
}



-----

curl -X POST https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
  -d 'grant_type=client_credentials' \
  -d 'client_id=<your-sp-client-id>' \
  -d 'client_secret=<your-sp-client-secret-value>' \
  -d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default'





curl -X POST https://<workspace-url>/oidc/v1/token \
-d 'grant_type=client_credentials' \
-d 'client_id=<your-sp-client-id>' \
-d 'client_secret=<your-sp-client-secret-value>' \
-d 'scope=all-apis'



----------

To achieve a generic JDBC URL for Databricks SQL Warehouses during cross-region disaster recovery (DR) testing or failover (e.g., from East US to West US), the core challenge is that both the workspace hostname (e.g., adb-1234567890123456.7.azuredatabricks.net) and the HTTP path (e.g., /sql/1.0/warehouses/<warehouse-id>) are region- and workspace-specific. Warehouse IDs are auto-generated UUIDs and will differ between primary (East) and secondary (West) workspaces, even if you replicate configurations identically.
Databricks does not provide built-in automatic failover or URL abstraction for workspaces/SQL Warehouses across regions. By default, consumers like IBM DataStage (via JDBC/ODBC) or SQL Server VMs (using linked servers or DSN configurations) would require URL updates during failover, which defeats your goal of no client-side changes.
The recommended approach is to front the Databricks endpoints with a cloud load balancer/CDN that provides a stable, generic URL. This routes traffic to the active region's workspace and handles path differences via dynamic rewriting/redirects. In Azure, use Azure Front Door (a global CDN/load balancer) for this, as it supports health-based routing, custom domains, and rule-based URL rewrites. This ensures clients use a fixed JDBC URL like jdbc:databricks://stable-dr-endpoint.azurefd.net:443;httpPath=/generic-warehouse.
Prerequisites

Two Azure Databricks workspaces: Primary in East US, secondary in West US.
Replicate data and assets:
Data: Use Delta Deep Clone for tables, or Azure Data Factory/Storage replication for other assets.
Configurations: Use Terraform (with the Databricks provider and Exporter tool) or DBSync to mirror notebooks, jobs, users/groups (via SCIM), ACLs, secrets, and SQL Warehouse configs.

SQL Warehouses: Create identical warehouses in both regions (same size, scaling, etc.), but note their unique IDs (e.g., primary: abc123, secondary: def456).
Authentication: Use OAuth (Entra ID service principal or user tokens) instead of PATs, as PATs are workspace-specific. Grant the identity CAN USE on warehouses in both regions.
Azure Front Door: Provision one in a global configuration (not region-bound).

Step-by-Step Setup for Generic JDBC URL

Provision Azure Front Door:
In the Azure portal, create a Front Door profile (Standard or Premium tier for rewrite rules).
Add a custom domain (e.g., stable-dr-endpoint.yourdomain.com) with a TLS cert (use Azure-managed or upload your own).
Create two origin groups (backends):
Primary: Point to the East workspace hostname (e.g., adb-primary.eastus.azuredatabricks.net), port 443, HTTPS.
Secondary: Point to the West workspace hostname (e.g., adb-secondary.westus.azuredatabricks.net), port 443, HTTPS.

Set up health probes: Use path /api/2.0/sql/warehouses (or similar) to check if the backend is healthy.
Default routing: Route all traffic to the primary origin group initially. Enable session affinity if needed for sticky connections.

Handle HTTP Path Differences with URL Rewrite Rules:
Warehouse IDs differ, so use Front Door's rules engine to dynamically rewrite the path.
Store active mappings in Azure Key Vault or App Configuration (e.g., a secret with JSON: {"active_host": "adb-primary.eastus.azuredatabricks.net", "active_warehouse_id": "abc123"}).
Create a rule set:
Match on incoming path: /generic-warehouse/* (your generic path).
Action: URL rewrite to /sql/1.0/warehouses/{active_warehouse_id}{request_uri} (use variables or integrate with Azure Functions for dynamic lookup from Key Vault).
For more advanced dynamics, attach an Azure Function to the rule: The function queries Key Vault for the current active ID and rewrites the path (e.g., replace /generic-warehouse with /sql/1.0/warehouses/abc123).

Route to the active origin based on health or manual switch.

Configure the Generic JDBC URL:
Use this format for all clients:textjdbc:databricks://stable-dr-endpoint.yourdomain.com:443;httpPath=/generic-warehouse;transportMode=http;ssl=1;AuthMech=11;Auth_Flow=0;Auth_AccessToken=<oauth-token>
stable-dr-endpoint.yourdomain.com: Your Front Door custom domain (resolves to the active backend).
/generic-warehouse: A placeholder path that Front Door rewrites to the actual /sql/1.0/warehouses/<id>.
Add other properties as needed (e.g., for OAuth: Acquire token via MSAL or azure-identity, as in prior examples).

For ODBC (e.g., in SQL Server linked servers or DSN):
Driver: Databricks ODBC driver (download from Databricks).
Server: stable-dr-endpoint.yourdomain.com
Port: 443
HTTP Path: /generic-warehouse
Authentication: OAuth or token.


Integrate with Consumers:
IBM DataStage: Update JDBC stages to use the generic URL. Test connections.
SQL VM Linked Server: In SSMS, create/modify the linked server with provider MSDASQL (for ODBC) or direct JDBC if using a bridge. Set server name to the generic hostname and path.
DSN: Create system DSNs on VMs pointing to the generic URL.

Failover Process (for DR Event or Testing):
Declare DR: Stop primary warehouse/jobs, stabilize data (e.g., flush streams).
Update Front Door:
Switch the default origin group to secondary (manual or via Azure CLI/PowerShell script).
Update Key Vault with secondary details (e.g., {"active_host": "adb-secondary.westus.azuredatabricks.net", "active_warehouse_id": "def456"}).

Rerun replication if needed (e.g., sync any delta changes).
Test: Clients should connect seamlessly without config changes—the Front Door handles routing and path rewrite.
Failback: Reverse the process after primary recovery.
Automate: Use Azure Logic Apps or Functions to trigger failover based on alerts (e.g., from Azure Monitor on region outage).


Testing and Considerations

DR Testing: Simulate failover by manually switching Front Door origins. Run queries from DataStage/SQL VM to verify no downtime or config changes.
Performance: Front Door adds minimal latency (~50-100ms globally). Use caching rules if applicable (though JDBC traffic isn't cacheable).
Costs: Front Door charges per GB transferred + requests. Estimate with Azure Pricing Calculator.
Security: Enable WAF on Front Door. Ensure OAuth tokens have minimal scopes.
Limitations: If using PATs, generate new ones post-failover (but prefer OAuth). Complex queries might need tuning if latency increases.
Reference Implementation: For Terraform code and examples (adapt for Azure Front Door), see https://github.com/gregwood-db/dbx-stable-url.

This setup ensures zero client changes during DR, with all logic handled centrally in Azure Front Door. If you encounter issues (e.g., with rewrite rules), share error details for refinement.




------------


Workspace-Level Permissions
Can Manage — Full control, including changing permissions and deleting.

Unity Catalog Privileges
MANAGE privilege — Allows granting/revoking privileges without full ownership.


Main Privilege Types by Securable Object
From official Databricks documentation (as of early 2026):

Metastore (top-level): CREATE CATALOG, CREATE EXTERNAL LOCATION, CREATE STORAGE CREDENTIAL, CREATE SHARE, USE MARKETPLACE ASSETS, USE SHARE, etc.
Catalog: ALL PRIVILEGES, USE CATALOG, BROWSE, CREATE SCHEMA, APPLY TAG, and inheritance for child objects (e.g., CREATE TABLE, USE SCHEMA).
Default: All users typically have USE CATALOG on main/workspace catalogs.

Schema: USE SCHEMA, CREATE TABLE, CREATE VOLUME, CREATE MODEL, CREATE FUNCTION, CREATE MATERIALIZED VIEW, and inheritance (e.g., SELECT, MODIFY on child tables).
Table / View / Materialized View: SELECT (read), MODIFY (write/insert/update/delete), ALL PRIVILEGES.
Volume (for files/unstructured data): READ VOLUME, WRITE VOLUME.
Function / Model / Procedure: EXECUTE.


Required RBAC role (Entra ID):
Storage Blob Data Contributor — Sufficient for creating directories, uploading files, etc. (includes write on blobs/directories).





----------

SELECT current_timestamp() AS current_datetime;


--

Recommended: Install via Cluster Libraries (Persists for All Notebooks/Jobs on Cluster)

Go to your cluster configuration → Libraries tab.
Click Install New → Maven (easiest and auto-handles dependencies).
In the "Coordinates" field, enter:textcom.databricks:databricks-jdbc:3.1.1(Check latest version on Maven Central — as of early 2026, 3.1.1 is recent; use 2.6.32 or similar if you need compatibility with older setups.)
Install → restart the cluster.

This adds the JAR to the full driver/executor classpath automatically — no code changes needed.



------------

# In East region notebook

# ── Replace these ────────────────────────────────────────────────────────────
west_jdbc_url = "jdbc:databricks://<your-west-server-hostname>:443/default"  # from above
http_path     = "/sql/1.0/warehouses/<your-warehouse-id>"
pat           = dbutils.secrets.get("your-scope", "west-pat")   # ← use secrets!

full_jdbc_url = f"{west_jdbc_url};transportMode=http;ssl=1;httpPath={http_path};AuthMech=3;UID=token;PWD={pat}"

try:
    # Test simple read from a system table (or any table you have access to in West)
    df = spark.read \
        .format("jdbc") \
        .option("url", full_jdbc_url) \
        .option("dbtable", "(SELECT current_version()) AS t") \
        .option("driver", "com.databricks.client.jdbc.Driver") \
        .load()

    display(df)

    print("Connection SUCCESS: East → West SQL Warehouse works!")

except Exception as e:
    print("Connection FAILED:")
    print(str(e))



---------

# === Variables - fill these in ===
$driverName       = "Simba Spark ODBC Driver"          # Or exact name from your ODBC drivers list
$hostName         = "adb-1234567890123456.7.azuredatabricks.net"   # Your workspace host
$httpPath         = "/sql/1.0/warehouses/abc1234567890def"         # From your SQL warehouse or cluster
$clientId         = "your-service-principal-application-id"       # UUID from Azure App reg / Databricks SP
$clientSecret     = "your-oauth-secret-from-databricks-sp"        # OAuth secret created in Databricks for this SP
$ssl              = "1"
$thriftTransport  = "2"   # HTTP/2 usually

# Build DSN-less connection string
$connString = @"
Driver={$driverName};
Host=$hostName;
Port=443;
HTTPPath=$httpPath;
SSL=$ssl;
ThriftTransport=$thriftTransport;
AuthMech=11;
Auth_Flow=1;
Auth_Client_Id=$clientId;
Auth_Client_Secret=$clientSecret
"@ -replace "`r`n", ""   # Make it single-line (ODBC requires no breaks)

Write-Host "Connection String:"
Write-Host $connString
Write-Host "`n"

# === Test connection using .NET ODBC (System.Data.Odbc) ===
Add-Type -AssemblyName System.Data

try {
    $conn = New-Object System.Data.Odbc.OdbcConnection
    $conn.ConnectionString = $connString
    $conn.Open()
    Write-Host "Connection SUCCESSFUL!" -ForegroundColor Green
    
    # Optional: run a quick test query
    $cmd = $conn.CreateCommand()
    $cmd.CommandText = "SELECT current_date() AS today"
    $reader = $cmd.ExecuteReader()
    while ($reader.Read()) {
        Write-Host "Test query result: $($reader["today"])"
    }
    $reader.Close()
}
catch {
    Write-Host "Connection FAILED: $($_.Exception.Message)" -ForegroundColor Red
    Write-Host $_.Exception | Format-List -Force
}
finally {
    if ($conn.State -eq "Open") { $conn.Close() }
}






-------

Install on the cluster (for all notebooks/jobs on that cluster)
Go to your cluster → Libraries tab
Click "Install New" → PyPI → enter azure-identity → Install
This persists across notebook restarts / job runs on the same cluster.

In Azure Databricks, it should pick up the system-assigned managed identity of the cluster (if enabled).
Cluster must have managed identity enabled:
Go to cluster configuration → Advanced options → Azure → Managed identity section.
Turn it on (system-assigned) or attach a user-assigned one.
The identity must have the necessary Microsoft Graph permissions (e.g., Application.Read.All) and admin consent granted in Entra ID.


%pip install azure-identity


from azure.identity import DefaultAzureCredential
import requests

# Acquire token using managed identity (works automatically on cluster with MSI enabled)
credential = DefaultAzureCredential()
token = credential.get_token("https://graph.microsoft.com/.default").token




object_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  # the SP object ID you have

url = f"https://graph.microsoft.com/v1.0/servicePrincipals/{object_id}"
# Or minimal version: f"https://graph.microsoft.com/v1.0/servicePrincipals/{object_id}?$select=displayName,id,appId"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}

response = requests.get(url, headers=headers)

if response.status_code == 200:
    sp_data = response.json()
    display_name = sp_data.get("displayName", "Not found")
    app_id = sp_data.get("appId")
    print(f"Service Principal Name: {display_name}")
    print(f"Application (Client) ID: {app_id}")
    print(sp_data)  # full object
else:
    print(f"Error {response.status_code}: {response.text}")



--



import json
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()   # uses the current notebook's authentication context (no extra token needed)

me = w.current_user.me()
print(json.dumps(me.as_dict(), indent=2))



--------

from databricks.sdk import WorkspaceClient
w = WorkspaceClient()
sp = w.service_principals.get(id="your-sp-internal-id")
print(sp.display_name)



---


from databricks.sdk import WorkspaceClient

w = WorkspaceClient()   # automatically uses the current notebook's authentication context

# Replace with your service principal ID (UUID string)
sp_id = "12345678-1234-1234-1234-1234567890ab"

sp = w.service_principals.get(id=sp_id)

print(f"Display name: {sp.display_name}")
print(f"Application ID: {sp.application_id}")
print(f"Active: {sp.active}")
print(f"Full object:\n{sp}")



import requests
import json

sp_id = "your-sp-id-here"          # ← replace

workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}

url = f"{workspace_url}/api/2.0/preview/scim/v2/ServicePrincipals/{sp_id}"

response = requests.get(url, headers=headers)

if response.status_code == 200:
    sp_data = response.json()
    print("Service Principal Name / Display Name:", sp_data.get("displayName"))
    print("Application ID:", sp_data.get("applicationId"))
    print(json.dumps(sp_data, indent=2))
else:
    print(f"Error {response.status_code}: {response.text}")




--------


from databricks.sdk import WorkspaceClient
from databricks.sdk.service.clusters import ClusterState
import time
from datetime import datetime

w = WorkspaceClient()  # ← no args = uses job's identity automatically

cluster_id = dbutils.widgets.get("cluster_id") or "your-cluster-id-here"

print(f"Restarting cluster {cluster_id} as {datetime.now()}")

w.clusters.restart(cluster_id=cluster_id)

# Optional wait loop (as before)
start = time.time()
while True:
    state = w.clusters.get(cluster_id=cluster_id).state
    print(f"State: {state}")
    if state == ClusterState.RUNNING:
        print("Restart complete ✓")
        break
    if state in [ClusterState.TERMINATED, ClusterState.ERROR]:
        raise RuntimeError(f"Failed: {state}")
    if time.time() - start > 20*60:
        print("Timeout")
        break
    time.sleep(30)



------------

import requests

token = dbutils.secrets.get(scope="your-scope", key="databricks-token")   # or use notebook context
host = spark.conf.get("spark.databricks.workspaceUrl")   # or hardcode https://<your-workspace>.cloud.databricks.com

headers = {"Authorization": f"Bearer {token}"}
payload = {"cluster_id": cluster_id}

r = requests.post(f"{host}/api/2.0/clusters/restart", json=payload, headers=headers)

if r.status_code != 200:
    print("Error:", r.status_code, r.text)
    # 403 → permission denied
    # 400/404 → wrong cluster_id
else:
    print("Restart accepted")



--------


# Databricks notebook source
# COMMAND ----------

dbutils.widgets.text("cluster_id", "", "Cluster ID to restart")
dbutils.widgets.dropdown("wait_for_restart", "true", ["true", "false"], "Wait until cluster is running again?")

# COMMAND ----------

cluster_id = dbutils.widgets.get("cluster_id").strip()
wait_for_restart = dbutils.widgets.get("wait_for_restart").lower() == "true"

if not cluster_id:
    raise ValueError("Parameter 'cluster_id' is required. You must pass the cluster ID.")

print(f"Target cluster to restart: {cluster_id}")

# COMMAND ----------

import time
from datetime import datetime

print(f"Restart initiated — {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Restart the cluster
dbutils.api("2.0").clusters.restart(cluster_id)

print("Restart command sent.")

# COMMAND ----------

if not wait_for_restart:
    print("Not waiting for cluster to come back online (as requested).")
    dbutils.notebook.exit("Restart command sent — wait disabled")

# COMMAND ----------

MAX_WAIT_MINUTES = 18
POLL_INTERVAL_SEC = 30

print(f"Waiting for cluster to reach RUNNING state (max {MAX_WAIT_MINUTES} min)...")

start = time.time()

while True:
    state = dbutils.api("2.0").clusters.get(cluster_id)["state"]
    
    print(f"  {datetime.now().strftime('%H:%M:%S')} → state = {state}")
    
    if state == "RUNNING":
        print("Cluster is RUNNING again ✓")
        break
        
    if state in ["TERMINATED", "ERROR", "UNKNOWN"]:
        raise Exception(f"Cluster entered terminal state: {state}")
    
    if (time.time() - start) > (MAX_WAIT_MINUTES * 60):
        print("Timeout reached — cluster did not start within time limit.")
        break
        
    time.sleep(POLL_INTERVAL_SEC)

print("Restart & wait procedure completed.")








------------

# Databricks notebook source
# COMMAND ----------

# SAMPLE: Unity Catalog → Azure SQL Managed Instance (using Azure Key Vault secrets)

# ─── 1. Read from Unity Catalog ────────────────────────────────────────

source_table = "main.default.my_source_table"          # ← change this
# or: source_table = dbutils.widgets.get("source_table")

df = spark.read.table(source_table)

print(f"Read {df.count():,} rows from {source_table}")
# display(df.limit(5))   # optional

# ─── Optional light transform ──────────────────────────────────────────
# df = df.filter("year >= 2024").drop("temp_column")

# ─── 2. Azure Key Vault backed secrets ─────────────────────────────────

# You must already have created a Key Vault-backed secret scope
# Example creation (one-time, outside notebook):
#   databricks secrets create-scope --scope my-company-kv-scope \
#     --keyvault-uri https://mycompanyvault.vault.azure.net/ \
#     --resource-id /subscriptions/.../providers/Microsoft.KeyVault/vaults/mycompanyvault

scope_name = "my-company-kv-scope"                     # ← your secret scope name

kv_username_key = "sql-mi-admin-user"                  # ← secret name in Key Vault
kv_password_key = "sql-mi-admin-password"              # ← secret name in Key Vault

jdbc_user     = dbutils.secrets.get(scope=scope_name, key=kv_username_key)
jdbc_password = dbutils.secrets.get(scope=scope_name, key=kv_password_key)

# ─── 3. Azure SQL Managed Instance connection settings ─────────────────

# ────────────────────── IMPORTANT ──────────────────────
# Get the correct hostname from Azure portal → SQL managed instance → Overview / Connection strings
# Public endpoint usually looks like:   yourinstance.public.abc123.database.windows.net
# Private endpoint:                     yourinstance.privatelink.database.windows.net  (VNet injected)

jdbc_hostname = "yourinstance.public.abc123.database.windows.net"   # ← CHANGE
jdbc_port     = 1433
jdbc_database = "TargetDatabase"                                    # ← CHANGE
jdbc_table    = "dbo.TargetTable"                                   # schema.table – CHANGE

# JDBC URL – standard for SQL MI with encryption
jdbc_url = (
    f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};"
    f"database={jdbc_database};"
    "encrypt=true;"
    "trustServerCertificate=false;"
    "hostNameInCertificate=*.database.windows.net;"
    "loginTimeout=30;"
)

connection_properties = {
    "user"                  : jdbc_user,
    "password"              : jdbc_password,
    "driver"                : "com.microsoft.sqlserver.jdbc.SQLServerDriver",
    "encrypt"               : "true",
    "trustServerCertificate": "false",
    "hostNameInCertificate" : "*.database.windows.net",
    "loginTimeout"          : "30"
}

# ─── 4. Write settings ─────────────────────────────────────────────────

write_mode = "append"           # "overwrite" | "append" | "errorifexists" | "ignore"

write_options = {
    "batchsize"     : "20000",           # tune depending on data volume & executor memory
    "isolationLevel": "READ_COMMITTED"   # or SERIALIZABLE / READ_UNCOMMITTED
}

# ─── 5. Execute write ──────────────────────────────────────────────────

df.write \
    .jdbc(
        url=jdbc_url,
        table=jdbc_table,
        mode=write_mode,
        properties={**connection_properties, **write_options}
    )

print(f"Write completed → {jdbc_table} ({write_mode} mode)")

# ─── Optional: repartition for better parallelism ──────────────────────
# Large tables: repartition before write
# df = df.repartition(24)   # adjust based on cluster size
# then .write.jdbc(...)



----------
# Read the table (works for Delta tables, Unity Catalog tables, etc.)
df = spark.read.table("catalog.schema.table_name")
# or: df = spark.table("catalog.schema.table_name")

# Alternative if reading by path:
# df = spark.read.format("delta").load("/path/to/delta/table")

# Write to CSV (creates a folder with multiple part-*.csv files + _SUCCESS)
df.write \
  .mode("overwrite") \
  .option("header", "true") \
  .csv("/dbfs/FileStore/my_export_folder/my_data")



-----------
SELECT 
  c.table_catalog          AS catalog_name,
  c.table_schema           AS schema_name,
  c.table_name,
  c.column_name,
  c.ordinal_position,
  lower(c.data_type)       AS data_type_lower,
  c.is_nullable,
  
  -- This is the key addition
  t.table_type             AS table_type,           -- 'BASE TABLE' or 'VIEW' (main values)
  
  -- Optional helpful extras from TABLES
  t.is_insertable_into,
  t.table_owner

FROM system.information_schema.columns  c

INNER JOIN system.information_schema.tables  t
  ON  c.table_catalog = t.table_catalog
  AND c.table_schema  = t.table_schema
  AND c.table_name    = t.table_name

WHERE c.table_catalog NOT IN ('system', 'samples')

-- Optional filters - uncomment/adjust as needed
-- AND c.table_catalog LIKE 'prod_%'               -- example prefix filter
-- ORDER BY c.table_catalog, c.table_schema, c.table_name, c.ordinal_position




---------------

SELECT * FROM OPENQUERY(DATABRICKS, 'SELECT CURRENT_CATALOG() AS default_catalog');



-------

# Widget input example: "sales_prod,finance_prod,hr_prod"
raw_input = dbutils.widgets.get("catalog_names").strip()   # better name for widget
catalog_list = [c.strip() for c in raw_input.split(",") if c.strip()]

if not catalog_list:
    raise ValueError("No catalog names provided")

print("Filtering exact catalogs:", catalog_list)

# Build IN clause
if catalog_list:
    quoted = ", ".join(f"'{c.replace('\'', '\'\'')}'" for c in catalog_list)
    catalog_filter = f" AND catalog_name IN ({quoted})"
else:
    catalog_filter = ""

-------------


# Cell: Configuration
dbutils.widgets.text("catalog_prefixes", "prod_sales, prod_finance, prod_hr", "Catalog prefixes (comma-separated)")
dbutils.widgets.text("compare_level", "table", "Compare level: catalog / schema / table / column")

raw_input = dbutils.widgets.get("catalog_prefixes").strip()
prefix_list = [p.strip() for p in raw_input.split(",") if p.strip()]

if not prefix_list:
    raise ValueError("No catalog prefixes provided")

print("Detected prefixes:", prefix_list)




# Updated get_uc_inventory function
def get_uc_inventory(level_filter="table", prefixes=None):
    """
    prefixes: list of strings like ['prod_sales', 'prod_finance']
    """
    exclude_catalogs = "'system','samples'"

    if prefixes:
        # Build LIKE conditions:  catalog_name LIKE 'prod_sales%' OR catalog_name LIKE 'prod_finance%' ...
        like_conditions = " OR ".join(
            f"catalog_name LIKE '{p.replace('\'', '\'\'')}%'" 
            for p in prefixes
        )
        catalog_filter = f" AND ({like_conditions})"
    else:
        catalog_filter = ""

    # ────────────────────────────────────────────────
    # catalog level
    # ────────────────────────────────────────────────
    if level_filter == "catalog":
        query = f"""
            SELECT catalog_name
            FROM system.information_schema.catalogs
            WHERE catalog_name NOT IN ({exclude_catalogs})
            {catalog_filter}
        """
        df = spark.sql(query)
        df = df.withColumn("key", col("catalog_name"))

    # ────────────────────────────────────────────────
    # schema level
    # ────────────────────────────────────────────────
    elif level_filter == "schema":
        query = f"""
            SELECT 
              catalog_name,
              schema_name
            FROM system.information_schema.schemata
            WHERE catalog_name NOT IN ({exclude_catalogs})
            {catalog_filter}
        """
        df = spark.sql(query)
        df = df.withColumn("key", concat_ws(".", col("catalog_name"), col("schema_name")))

    # ────────────────────────────────────────────────
    # table / column level
    # ────────────────────────────────────────────────
    else:
        tables_query = f"""
            SELECT 
              t.table_catalog        AS catalog_name,
              t.table_schema         AS schema_name,
              t.table_name,
              t.is_insertable_into,
              t.table_owner,
              CASE WHEN v.table_name IS NOT NULL THEN 'VIEW' ELSE 'TABLE' END AS object_type
            FROM system.information_schema.tables t
            LEFT JOIN system.information_schema.views v
              ON t.table_catalog = v.table_catalog
              AND t.table_schema  = v.table_schema
              AND t.table_name    = v.table_name
            WHERE t.table_catalog NOT IN ({exclude_catalogs})
            {catalog_filter}
        """

        df_tables = spark.sql(tables_query)

        columns_query = f"""
            SELECT 
              table_catalog        AS catalog_name,
              table_schema         AS schema_name,
              table_name,
              column_name,
              ordinal_position,
              lower(data_type)     AS data_type_lower,
              is_nullable
            FROM system.information_schema.columns
            WHERE table_catalog NOT IN ({exclude_catalogs})
            {catalog_filter}
        """

        df_columns = spark.sql(columns_query)

        if level_filter == "column":
            df = df_columns.withColumn(
                "key", 
                concat_ws(".", col("catalog_name"), col("schema_name"), col("table_name"), col("column_name"))
            ).select("key", "catalog_name", "schema_name", "table_name", "column_name", "data_type_lower", "is_nullable", "ordinal_position")
        else:
            df = df_tables.join(
                df_columns.groupBy("catalog_name", "schema_name", "table_name")
                          .agg(concat_ws("||", collect_list(concat_ws(":", col("column_name"), col("data_type_lower")))).alias("col_sig")),
                ["catalog_name", "schema_name", "table_name"], "left"
            ).withColumn("key", concat_ws(".", col("catalog_name"), col("schema_name"), col("table_name"))
            ).select("key", "catalog_name", "schema_name", "table_name", "object_type", "col_sig", "is_insertable_into", "table_owner")

    return df.cache()



if prefixes:
    quoted = ", ".join(f"'{p.replace('\'', '\'\'')}'" for p in prefixes)
    catalog_filter = f" AND catalog_name IN ({quoted})"




----------------

from pyspark.sql.functions import coalesce, col, count, sum, when

# Add the coalesced column first (or just use it inline)
summary = comparison.groupBy(
    coalesce(col("primary_catalog"), col("dr_catalog")).alias("catalog")
).agg(
    count("*").alias("total_compared"),
    sum(when(col("missing_in_dr"), 1).otherwise(0)).alias("missing_in_dr"),
    sum(when(col("missing_in_primary"), 1).otherwise(0)).alias("missing_in_primary"),
    sum(when(col("column_signature_mismatch"), 1).otherwise(0)).alias("sig_mismatch"),
    sum(when(col("type_mismatch"), 1).otherwise(0)).alias("type_mismatch")
)

display(summary)


-------

df = spark.table("my_catalog.my_schema.old_managed_sales")

df.write \
    .format("delta") \
    .mode("overwrite") \                    # or "append" if needed
    .option("path", "abfss://mycontainer@mystorageaccount.dfs.core.windows.net/curated/sales/daily/") \
    .saveAsTable("my_catalog.my_schema.new_external_sales")

----------------





-- Step 1: Create the new external table (empty) with LOCATION
-- Replace placeholders with your values
CREATE TABLE my_catalog.my_schema.new_external_sales
USING DELTA
LOCATION 's3://your-company-bucket/curated/sales/daily/'          -- must be covered by an external location
TBLPROPERTIES (
  'delta.enableChangeDataFeed' = 'true',                          -- optional
  'delta.autoOptimize.optimizeWrite' = 'true'
);

-- Step 2: Copy all data + schema from managed table
INSERT INTO my_catalog.my_schema.new_external_sales
SELECT * FROM my_catalog.my_schema.old_managed_sales;

-- Alternative one-liner (CTAS = CREATE TABLE AS SELECT)
CREATE TABLE my_catalog.my_schema.new_external_sales
USING DELTA
LOCATION 's3://your-company-bucket/curated/sales/daily/'
TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')
AS SELECT * FROM my_catalog.my_schema.old_managed_sales;



-----------

df_tables = spark.sql("""
    SELECT 
      table_catalog        AS catalog_name,
      table_schema         AS schema_name,
      table_name,
      is_insertable_into,
      table_owner,
      -- no table_type column exists
      CASE WHEN v.table_name IS NOT NULL THEN 'VIEW' ELSE 'TABLE' END AS object_type
    FROM system.information_schema.tables t
    LEFT JOIN system.information_schema.views v
      ON  t.table_catalog = v.table_catalog
      AND t.table_schema  = v.table_schema
      AND t.table_name    = v.table_name
    WHERE table_catalog NOT IN ('system','samples')
""")




# Cell 1 – Configuration
dbutils.widgets.text("primary_catalog_prefix", "prod_", "Primary catalog prefix")
dbutils.widgets.text("dr_catalog_prefix",     "dr_",   "DR catalog prefix")
dbutils.widgets.text("compare_level",         "table", "Compare level: catalog / schema / table / column")

primary_prefix = dbutils.widgets.get("primary_catalog_prefix")
dr_prefix      = dbutils.widgets.get("dr_catalog_prefix")
level          = dbutils.widgets.get("compare_level").lower()

print(f"Comparing catalogs starting with '{primary_prefix}' vs '{dr_prefix}'")


-- Cell 2 – Helper: show all catalogs (run in both regions to verify names)
SHOW CATALOGS;


# Cell 3 – Collect metadata from INFORMATION_SCHEMA (DR region)

from pyspark.sql.functions import col, concat_ws, lower

def get_uc_inventory(level_filter="table"):
    if level_filter == "catalog":
        df = spark.sql("SELECT catalog_name FROM system.information_schema.catalogs")
        df = df.withColumn("key", col("catalog_name"))
        
    elif level_filter == "schema":
        df = spark.sql("""
            SELECT 
              catalog_name,
              schema_name
            FROM system.information_schema.schemata
            WHERE catalog_name NOT IN ('system','samples')
        """)
        df = df.withColumn("key", concat_ws(".", col("catalog_name"), col("schema_name")))
        
    else:  # table + column level
        df_tables = spark.sql("""
            SELECT 
              table_catalog        AS catalog_name,
              table_schema         AS schema_name,
              table_name,
              table_type,
              is_insertable_into,
              created
            FROM system.information_schema.tables
            WHERE table_catalog NOT IN ('system','samples')
              AND table_type NOT IN ('VIEW')          -- optional: include/exclude views
        """)
        
        df_columns = spark.sql("""
            SELECT 
              table_catalog        AS catalog_name,
              table_schema         AS schema_name,
              table_name,
              column_name,
              ordinal_position,
              lower(data_type)     AS data_type_lower,
              is_nullable
            FROM system.information_schema.columns
            WHERE table_catalog NOT IN ('system','samples')
        """)
        
        if level_filter == "column":
            df = df_columns.withColumn(
                "key", 
                concat_ws(".", 
                          col("catalog_name"), 
                          col("schema_name"), 
                          col("table_name"), 
                          col("column_name"))
            ).select(
                "key", "catalog_name", "schema_name", "table_name", 
                "column_name", "data_type_lower", "is_nullable", "ordinal_position"
            )
        else:  # table level
            df = df_tables.join(
                df_columns.groupBy("catalog_name","schema_name","table_name")
                          .agg(concat_ws("||", 
                                         collect_list(concat_ws(":", 
                                                                col("column_name"), 
                                                                col("data_type_lower"))).alias("col_sig"))
            ).withColumn("key", concat_ws(".", 
                                         col("catalog_name"), 
                                         col("schema_name"), 
                                         col("table_name"))
            ).select("key", "catalog_name", "schema_name", "table_name", 
                     "table_type", "col_sig")
    
    return df

# Get DR inventory
dr_inventory = get_uc_inventory(level)
dr_inventory.createOrReplaceTempView("dr_inventory")
display(dr_inventory.limit(20))


# Cell 4 – Load primary inventory 
#   → Assumption: you previously exported it from primary region 
#     and saved it as Delta table or CSV in location accessible from DR

primary_path = "s3a://your-company-dr-bucket/uc-inventory/primary_metadata.parquet"   # ← change

primary_inventory = spark.read.format("delta").load(primary_path)   # or .csv(...)
primary_inventory.createOrReplaceTempView("primary_inventory")
display(primary_inventory.limit(20))


# Cell 5 – Compare

from pyspark.sql.functions import coalesce, lit

comparison = primary_inventory.alias("p").join(
    dr_inventory.alias("d"),
    "key",
    "full_outer"
).select(
    coalesce(col("p.key"), col("d.key")).alias("object_key"),
    col("p.catalog_name").alias("primary_catalog"),
    col("d.catalog_name").alias("dr_catalog"),
    col("p.schema_name").alias("primary_schema"),
    col("d.schema_name").alias("dr_schema"),
    
    # for table level
    col("p.table_name").alias("primary_table"),
    col("d.table_name").alias("dr_table"),
    col("p.table_type").alias("primary_table_type"),
    col("d.table_type").alias("dr_table_type"),
    col("p.col_sig").alias("primary_column_sig"),
    col("d.col_sig").alias("dr_column_sig"),
    
    # for column level (if chosen)
    col("p.column_name").alias("primary_col"),
    col("d.column_name").alias("dr_col"),
    col("p.data_type_lower").alias("primary_dtype"),
    col("d.data_type_lower").alias("dr_dtype"),
    
    # difference flags
    (col("p.key").isNull()).alias("missing_in_primary"),
    (col("d.key").isNull()).alias("missing_in_dr"),
    (col("p.col_sig") != col("d.col_sig")).alias("column_signature_mismatch"),
    (lower(col("p.data_type_lower")) != lower(col("d.data_type_lower"))).alias("type_mismatch")
)

# Show differences
diffs = comparison.filter(
    col("missing_in_primary") | 
    col("missing_in_dr") | 
    col("column_signature_mismatch") |
    col("type_mismatch")
)

print(f"Total objects in primary : {primary_inventory.count()}")
print(f"Total objects in DR      : {dr_inventory.count()}")
print(f"Differences found        : {diffs.count()}")

display(diffs.orderBy("object_key"))




# Cell 6 – Optional: Summary stats per catalog/schema

summary = comparison.groupBy(
    coalesce(col("primary_catalog"), col("dr_catalog")).alias("catalog")
).agg(
    count("*").alias("total_compared"),
    sum(when(col("missing_in_dr"),1).otherwise(0)).alias("missing_in_dr"),
    sum(when(col("missing_in_primary"),1).otherwise(0)).alias("missing_in_primary"),
    sum(when(col("column_signature_mismatch"),1).otherwise(0)).alias("sig_mismatch"),
    sum(when(col("type_mismatch"),1).otherwise(0)).alias("type_mismatch")
)

display(summary)



# Run in PRIMARY region notebook
primary_inventory = get_uc_inventory("table")   # or "column"
primary_inventory.write.mode("overwrite").format("delta").save("s3a://your-company-dr-bucket/uc-inventory/primary_metadata.parquet")































------------


from pyspark.sql.functions import col, lower

# Get list of sensitive full names (or table names) - broadcast small lookup
sensitive_list = [row.full_table_name for row in sensitive_df.select("full_table_name").distinct().collect()]
# or just table_name if you don't have full qualified: sensitive_list = [row.table_name ...]

# For better performance on small lookup → use broadcast hint or collect to array
sensitive_broadcast = sensitive_df.select("full_table_name").distinct()

# Filter df1: keep rows where statement_text does NOT contain ANY sensitive full name
df_filtered = df1.join(
    sensitive_broadcast.hint("broadcast"),
    lower(col("statement_text")).contains(lower(col("full_table_name"))),
    "left_anti"   # ← key: left anti join = rows from left with NO match on condition
)

df_filtered.show(truncate=False)



---------------------


-- 1. Create the lookup table (one row per sensitive table)
CREATE TABLE IF NOT EXISTS governance.sensitive_monitoring.sensitive_tables (
  catalog_name     STRING NOT NULL COMMENT 'e.g., main, finance, hr',
  schema_name      STRING NOT NULL COMMENT 'e.g., prod, staging',
  table_name       STRING NOT NULL COMMENT 'table name without catalog/schema',
  full_table_name  STRING GENERATED ALWAYS AS (CONCAT(catalog_name, '.', schema_name, '.', table_name)) 
    STORED AS STRING COMMENT 'For easy joins / lookups',
  sensitivity_level STRING COMMENT 'e.g., PII, PHI, Financial, Confidential, Restricted',
  owner_team       STRING COMMENT 'Owning team for follow-up',
  last_reviewed_date DATE COMMENT 'When last governance review occurred',
  notes            STRING COMMENT 'Additional context or risk notes',
  is_active        BOOLEAN DEFAULT TRUE COMMENT 'Set to FALSE if no longer sensitive'
)
USING DELTA
LOCATION 'abfss://your-container@yourstorage.dfs.core.windows.net/sensitive_monitoring/'   -- optional, for managed location
COMMENT 'Lookup of tables considered sensitive - used to scan query.history';

-- 2. Add some example data (replace with your real sensitive tables)
INSERT INTO governance.sensitive_monitoring.sensitive_tables
  (catalog_name, schema_name, table_name, sensitivity_level, owner_team, last_reviewed_date, notes)
VALUES
  ('main',      'finance',   'customer_payments',    'Financial',  'Finance Team',   '2026-01-15', 'Contains credit card tokens & amounts'),
  ('hr',        'prod',      'employee_personal',     'PII',        'HR',             '2026-01-10', 'SSN, DOB, address'),
  ('sales',     'staging',   'leads_with_emails',     'PII',        'Marketing',      '2025-12-20', 'Email + phone'),
  ('compliance','audit',     'audit_logs_raw',        'Confidential','Compliance',    '2026-02-01', 'Internal audit trail')
;

-- 3. (Optional) Add primary key / unique constraint for data quality
ALTER TABLE governance.sensitive_monitoring.sensitive_tables 
  ADD CONSTRAINT pk_sensitive_tables PRIMARY KEY (full_table_name);



-------------

EXEC master.dbo.sp_addlinkedserver 
    @server         = N'DATABRICKS_UAMI',              -- Your chosen linked server name
    @srvproduct     = N'Databricks',
    @provider       = N'MSDASQL',                      -- OLE DB Provider for ODBC
    @datasrc        = NULL,                            -- DSN-less, so blank
    @provstr        = N'Driver={Simba Spark ODBC Driver};'
                    + N'Host=adb-1234567890123456.7.azuredatabricks.net;'  -- ← your hostname
                    + N'Port=443;'
                    + N'HTTPPath=/sql/1.0/warehouses/your-warehouse-id;'   -- ← your HTTP Path
                    + N'SSL=1;'
                    + N'ThriftTransport=2;'
                    + N'SparkServerType=3;'            -- 3 = SQL Warehouse
                    + N'AuthMech=11;'                  -- OAuth 2.0
                    + N'Auth_Flow=3;'                  -- Azure Managed Identity flow
                    + N'Auth_Client_ID=123e4567-e89b-12d3-a456-426614174000;'  -- ← YOUR UAMI Client ID here
                    + N'Azure_workspace_resource_id=/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/your-rg/providers/Microsoft.Databricks/workspaces/your-ws-name;'
                    + N'Catalog=main;';                -- Optional: default catalog

-- No sp_addlinkedsrvlogin needed for managed identity flow (no UID/PWD)
-- The driver acquires token automatically via IMDS


-------------

from pyspark.sql.functions import col, explode, regexp_extract_all, lower, regexp_replace
import re

# Step 1: Fetch recent queries (adjust filters)
df_queries = spark.sql("""
  SELECT 
    statement_id,
    executed_by,
    start_time,
    statement_type,
    statement_text
  FROM system.query.history
  WHERE start_time >= current_timestamp() - INTERVAL 7 DAY
    AND statement_type IN ('SELECT', 'INSERT', 'UPDATE', 'MERGE', 'DELETE')  -- focus on DML/DQL
    AND statement_text IS NOT NULL
  ORDER BY start_time DESC
  LIMIT 500   -- for testing; remove in prod
""")

# Step 2: Clean text a bit (remove comments, extra whitespace)
df_clean = df_queries.withColumn(
    "clean_text",
    regexp_replace(lower(col("statement_text")), r'--.*?$|/\*.*?\*/|\s+', ' ')
)

# Step 3: Regex to find potential table references
# Pattern explanation:
#   \\b([a-z0-9_]+\\.)?([a-z0-9_]+\\.)?([a-z0-9_]+)\\b
#   → optional catalog. + optional schema. + table
#   Look for FROM/JOIN/INTO clauses roughly
pattern = r'(?:from|join|into|update|merge|describe|describe history)\s+([a-z0-9_]+(?:\.[a-z0-9_]+){0,2})\b'

df_extracted = df_clean.withColumn(
    "potential_refs",
    regexp_extract_all(col("clean_text"), lit(pattern), lit(1))
).withColumn(
    "table_refs", explode(col("potential_refs"))
).filter(col("table_refs").isNotNull())

# Step 4: Split into catalog/schema/table
df_parsed = df_extracted.withColumn(
    "parts", 
    split(regexp_replace(col("table_refs"), r'[^a-z0-9_\.]', ''), r'\.')
).withColumn(
    "num_parts", size(col("parts"))
).withColumn(
    "catalog",
    when(col("num_parts") == 3, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "schema",
    when(col("num_parts") == 3, element_at(col("parts"), 2))
      .when(col("num_parts") == 2, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "table",
    when(col("num_parts") == 3, element_at(col("parts"), 3))
      .when(col("num_parts") == 2, element_at(col("parts"), 2))
      .when(col("num_parts") == 1, element_at(col("parts"), 1))
      .otherwise(lit(None))
)

# Display results
display(df_parsed.select(
    "statement_id", "executed_by", "start_time", 
    "statement_text", "catalog", "schema", "table"
).orderBy(col("start_time").desc()))



-----------------

EXEC master.dbo.sp_addlinkedserver 
    @server         = N'DATABRICKS_DSNLESS',          -- Friendly name for the linked server (use in queries)
    @srvproduct     = N'Databricks SQL',
    @provider       = N'MSDASQL',                     -- Microsoft OLE DB Provider for ODBC Drivers
    @datasrc        = NULL,                           -- Leave blank for DSN-less
    @provstr        = N'Driver={Simba Spark ODBC Driver};'
                     + N'Host=adb-1234567890123456.7.azuredatabricks.net;'  -- ← your hostname
                     + N'Port=443;'
                     + N'HTTPPath=/sql/1.0/warehouses/abc123def456789;'     -- ← your HTTP Path
                     + N'SSL=1;'
                     + N'ThriftTransport=2;'           -- HTTP transport
                     + N'SparkServerType=3;'           -- 3 = SQL Warehouse (use 1 for interactive cluster)
                     + N'AuthMech=3;'                  -- 3 = User Name + Password (token)
                     + N'UID=token;'
                     + N'PWD=your_personal_access_token_here;'  -- ← your PAT (keep secure!)
                     + N'Catalog=main;';               -- Optional: sets default catalog (e.g. main, hive_metastore, or custom Unity Catalog)

-- Add security/login mapping (token-based, no Windows auth usually)
EXEC master.dbo.sp_addlinkedsrvlogin 
    @rmtsrvname = N'DATABRICKS_DSNLESS',
    @useself    = N'False',
    @locallogin = NULL,                       -- Applies to all local logins
    @rmtuser    = N'token',
    @rmtpassword = N'your_personal_access_token_here';  -- Same PAT as above


---------------

As a **Principal Architect** specializing in Azure data platforms, here's a clear, production-oriented guide to creating a **simple yet robust Azure Data Factory (ADF) pipeline** that reads multiple Parquet tables (folders/files) from **Azure Data Lake Storage Gen2 (ADLS Gen2)**.

This example assumes a common pattern:  
- Read Parquet files from a few source folders in ADLS Gen2 (Bronze/raw zone).  
- Optionally copy/transform them to another location (e.g., Silver zone, another container, or database).  
For simplicity, we'll copy the Parquet data as-is (or merge small files) to a target ADLS folder — a frequent starting point before adding transformations.

### Prerequisites (Everything You Need Before Starting)

| #  | Prerequisite                              | Details / Why Required                                                                 | Owner / Action Needed                          |
|----|-------------------------------------------|----------------------------------------------------------------------------------------|------------------------------------------------|
| 1  | Active Azure Subscription                 | ADF and ADLS Gen2 require an Azure subscription                                       | Subscription Owner / Create free trial if needed |
| 2  | Azure Data Factory instance (v2)          | The orchestration service. Use ADF in a region close to your storage for performance | Create via Azure Portal → Search "Data factories" |
| 3  | Azure Data Lake Storage Gen2 account      | Hierarchical namespace **must** be enabled (it's the default for new Gen2 accounts)   | Create Storage Account → Enable "Hierarchical namespace" |
| 4  | At least one container in ADLS Gen2       | e.g., `bronze`, `silver`                                                               | Create container(s) in the storage account     |
| 5  | Parquet files already present             | Folders like `/bronze/sales/year=2025/month=01/` with `.parquet` or `.snappy.parquet` files | Upload test files or use existing data         |
| 6  | Permissions (RBAC)                        | - **Contributor** or **Owner** on ADF & Storage resource groups<br>- Storage Blob Data Contributor / Reader on the ADLS account (or via ABAC) | Assign roles via Azure Portal → Access control (IAM) |
| 7  | Managed Identity or Access Key (recommended: Managed Identity) | ADF uses this to authenticate to ADLS Gen2 without secrets                             | Enable system-assigned managed identity on ADF |
| 8  | Browser + Azure Portal / ADF Studio       | Authoring UI (modern ADF authoring experience)                                         | Access via https://adf.azure.com               |
| 9  | (Optional but recommended)                | Azure Key Vault (for secrets if using account key instead of MI)                       | —                                              |
|10  | Basic familiarity with ADF concepts       | Linked Services, Datasets, Pipelines, Activities                                       | —                                              |

**Security note (2026 best practice)**:  
Always prefer **system-assigned managed identity** + **Storage Blob Data Contributor** role scoped to the container(s). Avoid account keys in production.

### Step-by-Step: Create a Simple ADF Pipeline to Read Parquet Files

#### Phase 1 – Prepare Linked Service & Datasets

1. **Open ADF Studio**  
   Azure Portal → Your Data Factory → Launch studio (or go directly to adf.azure.com)

2. **Create Linked Service to ADLS Gen2 (Source & Target can share one)**  
   - Manage tab → Linked services → New  
   - Search & select **Azure Data Lake Storage Gen2**  
   - Name: `LS_ADLS_Main` (or similar)  
   - Connect via **Managed Identity** (recommended) → Test connection  
     - If using account key: select Account key → paste from Storage Account → Keys  
   - Save

3. **Create Source Dataset (Parquet – parameterized for flexibility)**  
   - Author tab → Datasets → New dataset  
   - Azure Data Lake Storage Gen2 → Continue  
   - Format: **Parquet** → Continue  
   - Linked service: `LS_ADLS_Main`  
   - Name: `DS_Source_Parquet`  
   - **Parameters** tab: Add two parameters  
     - `Container` (String, default: `bronze`)  
     - `FolderPath` (String, default: `sales`)  
   - Connection tab → File path:  
     - Container = `@dataset().Container`  
     - Directory = `@dataset().FolderPath`  
   - Save

4. **Create Target Dataset (similar – Parquet sink)**  
   - New dataset → Azure Data Lake Storage Gen2 → Parquet  
   - Name: `DS_Target_Parquet`  
   - Linked service: same `LS_ADLS_Main`  
   - Parameters: `TargetContainer` (default: `silver`), `TargetFolder` (default: `processed/sales`)  
   - File path: use parameters like source  
   - (Optional) Compression: Snappy or none

#### Phase 2 – Build the Pipeline

5. **Create New Pipeline**  
   - Author tab → Pipelines → New pipeline  
   - Name: `PL_Read_Parquet_Tables`

6. **Add Copy Activity (one per table – or use ForEach for multiple)**  
   Simple version (3 tables = 3 copy activities):

   - Drag **Copy data** activity from Activities → Move & Transform  
   - Name: `Copy_Sales_Parquet`  
   - Source tab:  
     - Source dataset: `DS_Source_Parquet`  
     - Dataset properties:  
       - Container = `bronze`  
       - FolderPath = `sales/year=*/month=*`   (wildcard to read all partitions)  
   - Sink tab:  
     - Sink dataset: `DS_Target_Parquet`  
     - Dataset properties:  
       - TargetContainer = `silver`  
       - TargetFolder = `sales`  
     - Copy behavior: **PreserveHierarchy** (or FlattenHierarchy / MergeFiles if small files)  
   - Settings tab:  
     - Enable **Fault tolerance** if desired (skip incompatible rows)  
     - Logging level: Basic or Information

   Repeat for other tables (e.g., `customers`, `products`) by copying the activity and changing folder paths.

7. **(Better scalability) Use ForEach + Get Metadata for multiple tables**  
   - Add **Get Metadata** activity → get child folders from `/bronze`  
   - Add **ForEach** activity → items = `@activity('Get Metadata').output.childItems`  
   - Inside ForEach: Add Copy activity with dynamic content:  
     - FolderPath = `@item().name`

#### Phase 3 – Test & Publish

8. **Debug the pipeline**  
   - Click **Debug** → monitor progress  
   - Check output JSON, row counts, errors  
   - Validate files appeared in target folder

9. **Publish All**  
   - Click Publish → publish changes to factory

10. **Trigger (manual or schedule)**  
    - Add trigger: Manual / Schedule / Tumbling window  
    - For production: use **Event trigger** (blob created) or **Schedule trigger**

### Quick Architecture Summary (Medallion Style)

- **Bronze** (`bronze` container): raw Parquet as landed  
- **Silver** (`silver` container): cleaned / lightly transformed Parquet (this pipeline copies here)  
- Later add: Mapping Data Flow or Databricks for transformations

This pattern is used in production at scale — simple to maintain, cost-effective (serverless), and easy to extend with parameters, metadata-driven tables, or incremental logic.

If you need:
- Incremental load (watermark / last modified)
- Merge small files
- Load to Synapse / Fabric Lakehouse / SQL DB
- Error handling & alerting

...let me know — I can extend the design accordingly.



--------------------

from pyspark.sql.functions import (
    col, lower, regexp_extract_all, explode, 
    array_distinct, when, size, current_timestamp
)

# === Step 1: Read the system.query.history table ===
# Filter to recent queries (adjust interval as needed)
df = spark.table("system.query.history") \
    .where(col("query_start_time") >= current_timestamp() - expr("INTERVAL 7 DAYS")) \
    .where(col("statement_type") == "QUERY") \
    .where(col("statement_text").isNotNull()) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "statement_type",
        "executed_as_user_name"
    )

# === Step 2: Normalize and extract table-like patterns with regex ===
# Common patterns we want to catch:
#   catalog.schema.table
#   schema.table
#   table   (unqualified – harder to resolve without context)

df_with_extracted = df.withColumn(
    "normalized_sql", 
    lower(col("statement_text"))
).withColumn(
    # Extract anything that looks like a 1–3 part identifier after FROM/JOIN/INSERT/UPDATE/MERGE/etc.
    "potential_table_refs",
    regexp_extract_all(
        col("normalized_sql"),
        r"(?:from|join|insert\s+into|update|merge\s+into|create\s+or\s+replace\s+view|create\s+view|drop\s+table|alter\s+table)\s+([a-z0-9_]+\.[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+)\b",
        1
    )
).withColumn(
    # Also try to catch USE CATALOG / USE statements
    "used_catalog",
    regexp_extract_all(col("normalized_sql"), r"use\s+catalog\s+([a-z0-9_]+)", 1)
).withColumn(
    "used_schema",
    regexp_extract_all(col("normalized_sql"), r"use\s+([a-z0-9_]+)", 1)
)

# === Step 3: Explode the array of extracted tables for easier analysis ===
df_exploded = df_with_extracted \
    .withColumn("table_ref", explode(col("potential_table_refs"))) \
    .withColumn("table_ref_clean", 
        when(col("table_ref").contains("."), col("table_ref"))
         .otherwise(concat(lit("???.???."), col("table_ref")))  # mark unqualified
    ) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "table_ref_clean",
        size("potential_table_refs").alias("table_count"),
        "used_catalog",
        "used_schema"
    )

# === Step 4: Show results (deduplicate per statement if you want cleaner view) ===
display(
    df_exploded
    .orderBy(col("query_start_time").desc())
    .limit(200)
)

# Optional: Most accessed tables (approximate)
df_exploded.groupBy("table_ref_clean") \
    .agg(
        count("*").alias("access_count"),
        min("query_start_time").alias("first_seen"),
        max("query_start_time").alias("last_seen")
    ) \
    .orderBy(col("access_count").desc()) \
    .display()



--------------------

table_name = "your_catalog.your_schema.query_history_longterm"

# Get current columns (case-insensitive)
desc_df = spark.sql(f"DESCRIBE TABLE {table_name}")
existing_cols = {row["col_name"].lower(): row["col_name"] for row in desc_df.collect()}

columns_to_add = [
    ("ingestion_ts", "TIMESTAMP"),
    ("email_processed_flag", "BOOLEAN")
]

added = []

for col_name, col_type in columns_to_add:
    if col_name.lower() not in existing_cols:
        spark.sql(f"""
            ALTER TABLE {table_name}
            ADD COLUMNS ({col_name} {col_type})
        """)
        added.append(col_name)

if added:
    print(f"Added columns: {', '.join(added)}")
else:
    print("All required columns already exist — no changes made")





INSERT INTO governance.access.allowed_actors VALUES
('finance_readonly', 'USER', 'jane.doe@company.com', 'jane.doe@company.com', true, current_timestamp(), NULL, 'security-team', 'INC12345', NULL, current_timestamp(), current_user(), current_timestamp(), current_user()),
('finance_readonly', 'SPN',  '00000000-0000-0000-0000-000000000000', 'spn-finance-bi', true, current_timestamp(), NULL, 'security-team', 'INC12346', NULL, current_timestamp(), current_user(), current_timestamp(), current_user());



CREATE TABLE IF NOT EXISTS governance.access.allowed_actors (
  policy_name        STRING,
  actor_type         STRING,        -- 'USER' | 'SPN' | 'UAMI'
  actor_id           STRING,        -- stable ID if you have it (recommended)
  actor_name         STRING,        -- email or display name (optional but useful)
  active             BOOLEAN,
  effective_from     TIMESTAMP,
  effective_to       TIMESTAMP,
  approved_by        STRING,
  ticket             STRING,
  notes              STRING,
  created_at         TIMESTAMP,
  created_by         STRING,
  updated_at         TIMESTAMP,
  updated_by         STRING
)
USING DELTA;




SELECT
  q.statement_id,
  q.executed_by,
  q.start_time,
  t.catalog_name,
  t.schema_name,
  t.table_name,
  t.access_type   -- READ / WRITE
FROM system.query.history q
JOIN system.query.table_lineage t
  ON q.statement_id = t.statement_id
WHERE q.statement_type = 'SELECT';



statement_id
start_time
end_time
executed_by <--------------
executed_by_user_id
client_application
warehouse_name
execution_status
duration_ms
query_text

rows_read
rows_written





ALTER TABLE your_catalog.your_schema.query_history_longterm
ADD COLUMNS (
  ingestion_ts TIMESTAMP,
  email_processed_flag BOOLEAN
);




-- In your ingestion task (e.g. scheduled notebook %sql or pure SQL job)
INSERT INTO your_catalog.your_schema.query_history_longterm
SELECT 
  *, 
  current_timestamp() AS ingestion_ts,
  FALSE AS email_processed_flag
FROM system.query.history
WHERE start_time > (SELECT COALESCE(MAX(ingestion_ts), TIMESTAMP '1900-01-01') FROM your_catalog.your_schema.query_history_longterm)





from pyspark.sql.functions import col
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
delta_target = DeltaTable.forName(spark, target_table)

# Find unprocessed records
unprocessed = (
    spark.read.table(target_table)
    .filter((col("email_processed_flag") == False) | col("email_processed_flag").isNull())
    # Optional: add filters, e.g. .filter("execution_status = 'FAILED'") or time window
)

if unprocessed.count() > 0:
    # Your email logic here (example placeholder)
    # Loop or batch-send emails based on unprocessed rows
    # For each row: send email about failed/cancelled query, using statement_text, executed_by, error_message, etc.
    # Example: print for demo
    unprocessed.select("statement_id", "executed_by_user_name", "statement_text", "error_message").show(truncate=False)
    
    # Simulate sending (replace with real email code)
    print("Sending emails for above records...")

    # Update flag for processed rows (safe: only those we just read)
    # Option A: Simple update on the whole set (if all succeeded)
    delta_target.update(
        condition = (col("email_processed_flag") == False) | col("email_processed_flag").isNull(),
        set = {"email_processed_flag": lit(True)}
    )

    # Option B: If some emails fail, update only successful ones (more complex; track per-row)





from pyspark.sql.functions import col, current_timestamp, lit
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
source_table = "system.query.history"

# Get last ingested timestamp (from ingestion_ts now, more reliable than start_time alone)
max_ts_df = spark.sql(f"SELECT COALESCE(MAX(ingestion_ts), '1900-01-01') AS max_ts FROM {target_table}")
max_ts = max_ts_df.collect()[0]["max_ts"]

# Fetch new data with buffer
new_data = (
    spark.read.table(source_table)
    .filter(col("start_time") > max_ts)  # still use start_time to avoid re-fetching everything
    .withColumn("ingestion_ts", current_timestamp())
    .withColumn("email_processed_flag", lit(False))  # default unprocessed
)

if new_data.count() > 0:
    # Append (fastest, since statement_id + workspace_id should be unique)
    new_data.write.format("delta").mode("append").saveAsTable(target_table)

    # Or safer MERGE if you see duplicates occasionally
    # delta_target = DeltaTable.forName(spark, target_table)
    # delta_target.alias("target").merge(
    #     new_data.alias("source"),
    #     "target.statement_id = source.statement_id AND target.workspace_id = source.workspace_id"
    # ).whenNotMatchedInsertAll().execute()






--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import json

# === Retrieve credentials from Databricks secrets ===
client_id     = dbutils.secrets.get(scope="my-graph-scope", key="client-id")
tenant_id     = dbutils.secrets.get(scope="my-graph-scope", key="tenant-id")
client_secret = dbutils.secrets.get(scope="my-graph-scope", key="client-secret")

# === 1. Get access token (client credentials flow) ===
token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"

payload = {
    "client_id": client_id,
    "scope": "https://graph.microsoft.com/.default",
    "client_secret": client_secret,
    "grant_type": "client_credentials"
}

response = requests.post(token_url, data=payload)
response.raise_for_status()
access_token = response.json()["access_token"]
print("✅ Token acquired")

# === 2. Send email ===
user_principal_name = "sender@yourdomain.com"   # ← The mailbox you want to send FROM
graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/sendMail"

email_body = {
    "message": {
        "subject": "Test Email from Databricks",
        "body": {
            "contentType": "HTML",
            "content": "<h1>Hello from Databricks!</h1><p>This email was sent via Microsoft Graph API.</p>"
        },
        "toRecipients": [
            {
                "emailAddress": {
                    "address": "recipient@yourdomain.com"
                }
            }
        ],
        # Optional: ccRecipients, bccRecipients
        # Optional: attachments (base64 encoded)
    },
    "saveToSentItems": True
}

headers = {
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json"
}

response = requests.post(graph_url, headers=headers, json=email_body)

if response.status_code == 202:
    print("✅ Email accepted for delivery")
else:
    print(f"❌ Error: {response.status_code}")
    print(response.text)








# ─── Most recommended solution ───

client_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"   # ← the ID you see in executed_as

query = f"""
Resources
| where type == 'microsoft.managedidentity/userassignedidentities'
| where properties.clientId == '{client_id}'
| project 
    name,
    resourceGroup,
    subscriptionId,
    principalId,
    clientId,
    id,
    location
"""

from azure.identity import DefaultAzureCredential
from azure.mgmt.resourcegraph import ResourceGraphClient

credential = DefaultAzureCredential()

# If you're using a cluster with UAMI attached → it will be picked up automatically
# Otherwise you can explicitly pass client_id of UAMI you want to use for auth
# credential = DefaultAzureCredential(managed_identity_client_id="your-uami-client-id")

rg_client = ResourceGraphClient(credential)

result = rg_client.resources(
    resources = {
        "query": query,
        "subscriptions": []   # ← empty = search in all accessible subscriptions
    }
)

if result.data:
    for row in result.data:
        print(f"UAMI Name        : {row['name']}")
        print(f"Resource Group   : {row['resourceGroup']}")
        print(f"Client ID        : {row['clientId']}")
        print(f"Object ID        : {row['principalId']}")
        print("-" * 60)
else:
    print("No UAMI found with this client id")







from pyspark.sql.functions import regexp_replace, col

df_clean = df.withColumn(
    "sql_stmt_clean",
    regexp_replace(
        regexp_replace(
            regexp_replace(col("sql_stmt"), r"[\n\r]+", " "),    # newlines → single space
            r"\t+", " "),                                        # tabs → space
        r" +", " "                                               # multiple spaces → single space
    )
)

# Optional: trim + collapse whitespace even more aggressively
# .withColumn("sql_stmt_clean", trim(regexp_replace(col("sql_stmt_clean"), r"\s+", " ")))

display(df_clean.select("id", "sql_stmt_clean", "other_col").limit(20))






# Databricks notebook source
# COMMAND ----------

# ────────────────────────────────────────────────
#          CONFIGURATION - CHANGE THESE
# ────────────────────────────────────────────────

CATALOG_NAME      = "main"                     # or your catalog name
SCHEMA_NAME       = "sales"
TABLE_NAME        = "orders"

DATE_COLUMN       = "order_date"               # ← important: your timestamp/date column name
EMAIL_RECIPIENTS  = ["manager@company.com", "teamlead@company.com"]
EMAIL_SUBJECT     = "Daily Orders Summary – " + dbutils.widgets.text("today", "")

YOUR_SENDER_NAME  = "Databricks Alerts <alerts@yourcompany.com>"

# If you want to use SMTP instead of Databricks email → fill these
USE_SMTP          = False
SMTP_HOST         = "smtp.office365.com"
SMTP_PORT         = 587
SMTP_USER         = "your.email@company.com"
SMTP_PASSWORD     = dbutils.secrets.get("email-secrets", "smtp-password")  # ← recommended

# ────────────────────────────────────────────────
#               1. READ TODAY'S DATA
# ────────────────────────────────────────────────

from pyspark.sql.functions import col, current_date, to_date, lit
from datetime import date

today = date.today()
print(f"Processing date: {today}")

table_full_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

df_today = (
    spark.table(table_full_name)
    .where(to_date(col(DATE_COLUMN)) == lit(today))
    # .where(col(DATE_COLUMN).cast("date") == current_date())   # alternative
)

count = df_today.count()
print(f"Found {count:,} records for today")

if count == 0:
    dbutils.notebook.exit("No records found for today → stopping")

# COMMAND ----------

# ────────────────────────────────────────────────
#               2. BUILD NICE HTML TABLE
# ────────────────────────────────────────────────

# Optional: take a sample or aggregate — here we show raw records (limit to avoid huge emails)
display_limit = 500
pdf = df_today.limit(display_limit).toPandas()

# Convert to HTML with basic but clean styling
html_table = pdf.to_html(
    index=False,
    border=0,
    classes="table table-striped table-hover",
    justify="left"
)

html_content = f"""\
<html>
<head>
  <style>
    body {{ font-family: -apple-system, BlinkMacOSystemFont, 'Segoe UI', Roboto, sans-serif; color: #333; }}
    .header {{ background: #1e3a8a; color: white; padding: 16px; border-radius: 6px 6px 0 0; }}
    .content {{ padding: 20px; background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0 0 6px 6px; }}
    table {{ width: 100%; border-collapse: collapse; }}
    th, td {{ padding: 10px 12px; text-align: left; border-bottom: 1px solid #e5e7eb; }}
    th {{ background: #f3f4f6; font-weight: 600; }}
    .footer {{ margin-top: 24px; font-size: 0.9em; color: #6b7280; }}
  </style>
</head>
<body>
  <div class="header">
    <h2>Daily Orders Report – {today}</h2>
    <p>Found <strong>{count:,}</strong> new orders today</p>
  </div>
  
  <div class="content">
    {html_table}
    
    <div class="footer">
      {'Showing first ' + str(display_limit) + ' rows' if count > display_limit else 'All records shown'}<br>
      Full dataset available in Databricks → {table_full_name}
    </div>
  </div>
</body>
</html>
"""

# COMMAND ----------

# ────────────────────────────────────────────────
#               3. SEND EMAIL
# ────────────────────────────────────────────────

if not USE_SMTP:
    # === Preferred way in Databricks (no credentials needed) ===
    dbutils.notebook.run("/Shared/send_html_email", 120, {
        "subject": f"{EMAIL_SUBJECT} {today}",
        "html_body": html_content,
        "to": ",".join(EMAIL_RECIPIENTS),
        "from_name": YOUR_SENDER_NAME
    })
    print("Email sent using Databricks internal email service")

else:
    # === Classic SMTP way (Outlook 365 / Gmail etc) ===
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"{EMAIL_SUBJECT} {today}"
    msg["From"]    = YOUR_SENDER_NAME
    msg["To"]      = ", ".join(EMAIL_RECIPIENTS)

    msg.attach(MIMEText(html_content, "html"))

    try:
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        print("Email sent successfully via SMTP")
    except Exception as e:
        print("SMTP failed:", str(e))

# COMMAND ----------

# Optional: show preview in notebook
displayHTML(html_content[:32000])  # avoid notebook crash with very large tables
