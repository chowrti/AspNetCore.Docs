from pyspark.sql.functions import col, explode, regexp_extract_all, lower, regexp_replace
import re

# Step 1: Fetch recent queries (adjust filters)
df_queries = spark.sql("""
  SELECT 
    statement_id,
    executed_by,
    start_time,
    statement_type,
    statement_text
  FROM system.query.history
  WHERE start_time >= current_timestamp() - INTERVAL 7 DAY
    AND statement_type IN ('SELECT', 'INSERT', 'UPDATE', 'MERGE', 'DELETE')  -- focus on DML/DQL
    AND statement_text IS NOT NULL
  ORDER BY start_time DESC
  LIMIT 500   -- for testing; remove in prod
""")

# Step 2: Clean text a bit (remove comments, extra whitespace)
df_clean = df_queries.withColumn(
    "clean_text",
    regexp_replace(lower(col("statement_text")), r'--.*?$|/\*.*?\*/|\s+', ' ')
)

# Step 3: Regex to find potential table references
# Pattern explanation:
#   \\b([a-z0-9_]+\\.)?([a-z0-9_]+\\.)?([a-z0-9_]+)\\b
#   → optional catalog. + optional schema. + table
#   Look for FROM/JOIN/INTO clauses roughly
pattern = r'(?:from|join|into|update|merge|describe|describe history)\s+([a-z0-9_]+(?:\.[a-z0-9_]+){0,2})\b'

df_extracted = df_clean.withColumn(
    "potential_refs",
    regexp_extract_all(col("clean_text"), lit(pattern), lit(1))
).withColumn(
    "table_refs", explode(col("potential_refs"))
).filter(col("table_refs").isNotNull())

# Step 4: Split into catalog/schema/table
df_parsed = df_extracted.withColumn(
    "parts", 
    split(regexp_replace(col("table_refs"), r'[^a-z0-9_\.]', ''), r'\.')
).withColumn(
    "num_parts", size(col("parts"))
).withColumn(
    "catalog",
    when(col("num_parts") == 3, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "schema",
    when(col("num_parts") == 3, element_at(col("parts"), 2))
      .when(col("num_parts") == 2, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "table",
    when(col("num_parts") == 3, element_at(col("parts"), 3))
      .when(col("num_parts") == 2, element_at(col("parts"), 2))
      .when(col("num_parts") == 1, element_at(col("parts"), 1))
      .otherwise(lit(None))
)

# Display results
display(df_parsed.select(
    "statement_id", "executed_by", "start_time", 
    "statement_text", "catalog", "schema", "table"
).orderBy(col("start_time").desc()))



-----------------

EXEC master.dbo.sp_addlinkedserver 
    @server         = N'DATABRICKS_DSNLESS',          -- Friendly name for the linked server (use in queries)
    @srvproduct     = N'Databricks SQL',
    @provider       = N'MSDASQL',                     -- Microsoft OLE DB Provider for ODBC Drivers
    @datasrc        = NULL,                           -- Leave blank for DSN-less
    @provstr        = N'Driver={Simba Spark ODBC Driver};'
                     + N'Host=adb-1234567890123456.7.azuredatabricks.net;'  -- ← your hostname
                     + N'Port=443;'
                     + N'HTTPPath=/sql/1.0/warehouses/abc123def456789;'     -- ← your HTTP Path
                     + N'SSL=1;'
                     + N'ThriftTransport=2;'           -- HTTP transport
                     + N'SparkServerType=3;'           -- 3 = SQL Warehouse (use 1 for interactive cluster)
                     + N'AuthMech=3;'                  -- 3 = User Name + Password (token)
                     + N'UID=token;'
                     + N'PWD=your_personal_access_token_here;'  -- ← your PAT (keep secure!)
                     + N'Catalog=main;';               -- Optional: sets default catalog (e.g. main, hive_metastore, or custom Unity Catalog)

-- Add security/login mapping (token-based, no Windows auth usually)
EXEC master.dbo.sp_addlinkedsrvlogin 
    @rmtsrvname = N'DATABRICKS_DSNLESS',
    @useself    = N'False',
    @locallogin = NULL,                       -- Applies to all local logins
    @rmtuser    = N'token',
    @rmtpassword = N'your_personal_access_token_here';  -- Same PAT as above


---------------

As a **Principal Architect** specializing in Azure data platforms, here's a clear, production-oriented guide to creating a **simple yet robust Azure Data Factory (ADF) pipeline** that reads multiple Parquet tables (folders/files) from **Azure Data Lake Storage Gen2 (ADLS Gen2)**.

This example assumes a common pattern:  
- Read Parquet files from a few source folders in ADLS Gen2 (Bronze/raw zone).  
- Optionally copy/transform them to another location (e.g., Silver zone, another container, or database).  
For simplicity, we'll copy the Parquet data as-is (or merge small files) to a target ADLS folder — a frequent starting point before adding transformations.

### Prerequisites (Everything You Need Before Starting)

| #  | Prerequisite                              | Details / Why Required                                                                 | Owner / Action Needed                          |
|----|-------------------------------------------|----------------------------------------------------------------------------------------|------------------------------------------------|
| 1  | Active Azure Subscription                 | ADF and ADLS Gen2 require an Azure subscription                                       | Subscription Owner / Create free trial if needed |
| 2  | Azure Data Factory instance (v2)          | The orchestration service. Use ADF in a region close to your storage for performance | Create via Azure Portal → Search "Data factories" |
| 3  | Azure Data Lake Storage Gen2 account      | Hierarchical namespace **must** be enabled (it's the default for new Gen2 accounts)   | Create Storage Account → Enable "Hierarchical namespace" |
| 4  | At least one container in ADLS Gen2       | e.g., `bronze`, `silver`                                                               | Create container(s) in the storage account     |
| 5  | Parquet files already present             | Folders like `/bronze/sales/year=2025/month=01/` with `.parquet` or `.snappy.parquet` files | Upload test files or use existing data         |
| 6  | Permissions (RBAC)                        | - **Contributor** or **Owner** on ADF & Storage resource groups<br>- Storage Blob Data Contributor / Reader on the ADLS account (or via ABAC) | Assign roles via Azure Portal → Access control (IAM) |
| 7  | Managed Identity or Access Key (recommended: Managed Identity) | ADF uses this to authenticate to ADLS Gen2 without secrets                             | Enable system-assigned managed identity on ADF |
| 8  | Browser + Azure Portal / ADF Studio       | Authoring UI (modern ADF authoring experience)                                         | Access via https://adf.azure.com               |
| 9  | (Optional but recommended)                | Azure Key Vault (for secrets if using account key instead of MI)                       | —                                              |
|10  | Basic familiarity with ADF concepts       | Linked Services, Datasets, Pipelines, Activities                                       | —                                              |

**Security note (2026 best practice)**:  
Always prefer **system-assigned managed identity** + **Storage Blob Data Contributor** role scoped to the container(s). Avoid account keys in production.

### Step-by-Step: Create a Simple ADF Pipeline to Read Parquet Files

#### Phase 1 – Prepare Linked Service & Datasets

1. **Open ADF Studio**  
   Azure Portal → Your Data Factory → Launch studio (or go directly to adf.azure.com)

2. **Create Linked Service to ADLS Gen2 (Source & Target can share one)**  
   - Manage tab → Linked services → New  
   - Search & select **Azure Data Lake Storage Gen2**  
   - Name: `LS_ADLS_Main` (or similar)  
   - Connect via **Managed Identity** (recommended) → Test connection  
     - If using account key: select Account key → paste from Storage Account → Keys  
   - Save

3. **Create Source Dataset (Parquet – parameterized for flexibility)**  
   - Author tab → Datasets → New dataset  
   - Azure Data Lake Storage Gen2 → Continue  
   - Format: **Parquet** → Continue  
   - Linked service: `LS_ADLS_Main`  
   - Name: `DS_Source_Parquet`  
   - **Parameters** tab: Add two parameters  
     - `Container` (String, default: `bronze`)  
     - `FolderPath` (String, default: `sales`)  
   - Connection tab → File path:  
     - Container = `@dataset().Container`  
     - Directory = `@dataset().FolderPath`  
   - Save

4. **Create Target Dataset (similar – Parquet sink)**  
   - New dataset → Azure Data Lake Storage Gen2 → Parquet  
   - Name: `DS_Target_Parquet`  
   - Linked service: same `LS_ADLS_Main`  
   - Parameters: `TargetContainer` (default: `silver`), `TargetFolder` (default: `processed/sales`)  
   - File path: use parameters like source  
   - (Optional) Compression: Snappy or none

#### Phase 2 – Build the Pipeline

5. **Create New Pipeline**  
   - Author tab → Pipelines → New pipeline  
   - Name: `PL_Read_Parquet_Tables`

6. **Add Copy Activity (one per table – or use ForEach for multiple)**  
   Simple version (3 tables = 3 copy activities):

   - Drag **Copy data** activity from Activities → Move & Transform  
   - Name: `Copy_Sales_Parquet`  
   - Source tab:  
     - Source dataset: `DS_Source_Parquet`  
     - Dataset properties:  
       - Container = `bronze`  
       - FolderPath = `sales/year=*/month=*`   (wildcard to read all partitions)  
   - Sink tab:  
     - Sink dataset: `DS_Target_Parquet`  
     - Dataset properties:  
       - TargetContainer = `silver`  
       - TargetFolder = `sales`  
     - Copy behavior: **PreserveHierarchy** (or FlattenHierarchy / MergeFiles if small files)  
   - Settings tab:  
     - Enable **Fault tolerance** if desired (skip incompatible rows)  
     - Logging level: Basic or Information

   Repeat for other tables (e.g., `customers`, `products`) by copying the activity and changing folder paths.

7. **(Better scalability) Use ForEach + Get Metadata for multiple tables**  
   - Add **Get Metadata** activity → get child folders from `/bronze`  
   - Add **ForEach** activity → items = `@activity('Get Metadata').output.childItems`  
   - Inside ForEach: Add Copy activity with dynamic content:  
     - FolderPath = `@item().name`

#### Phase 3 – Test & Publish

8. **Debug the pipeline**  
   - Click **Debug** → monitor progress  
   - Check output JSON, row counts, errors  
   - Validate files appeared in target folder

9. **Publish All**  
   - Click Publish → publish changes to factory

10. **Trigger (manual or schedule)**  
    - Add trigger: Manual / Schedule / Tumbling window  
    - For production: use **Event trigger** (blob created) or **Schedule trigger**

### Quick Architecture Summary (Medallion Style)

- **Bronze** (`bronze` container): raw Parquet as landed  
- **Silver** (`silver` container): cleaned / lightly transformed Parquet (this pipeline copies here)  
- Later add: Mapping Data Flow or Databricks for transformations

This pattern is used in production at scale — simple to maintain, cost-effective (serverless), and easy to extend with parameters, metadata-driven tables, or incremental logic.

If you need:
- Incremental load (watermark / last modified)
- Merge small files
- Load to Synapse / Fabric Lakehouse / SQL DB
- Error handling & alerting

...let me know — I can extend the design accordingly.



--------------------

from pyspark.sql.functions import (
    col, lower, regexp_extract_all, explode, 
    array_distinct, when, size, current_timestamp
)

# === Step 1: Read the system.query.history table ===
# Filter to recent queries (adjust interval as needed)
df = spark.table("system.query.history") \
    .where(col("query_start_time") >= current_timestamp() - expr("INTERVAL 7 DAYS")) \
    .where(col("statement_type") == "QUERY") \
    .where(col("statement_text").isNotNull()) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "statement_type",
        "executed_as_user_name"
    )

# === Step 2: Normalize and extract table-like patterns with regex ===
# Common patterns we want to catch:
#   catalog.schema.table
#   schema.table
#   table   (unqualified – harder to resolve without context)

df_with_extracted = df.withColumn(
    "normalized_sql", 
    lower(col("statement_text"))
).withColumn(
    # Extract anything that looks like a 1–3 part identifier after FROM/JOIN/INSERT/UPDATE/MERGE/etc.
    "potential_table_refs",
    regexp_extract_all(
        col("normalized_sql"),
        r"(?:from|join|insert\s+into|update|merge\s+into|create\s+or\s+replace\s+view|create\s+view|drop\s+table|alter\s+table)\s+([a-z0-9_]+\.[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+)\b",
        1
    )
).withColumn(
    # Also try to catch USE CATALOG / USE statements
    "used_catalog",
    regexp_extract_all(col("normalized_sql"), r"use\s+catalog\s+([a-z0-9_]+)", 1)
).withColumn(
    "used_schema",
    regexp_extract_all(col("normalized_sql"), r"use\s+([a-z0-9_]+)", 1)
)

# === Step 3: Explode the array of extracted tables for easier analysis ===
df_exploded = df_with_extracted \
    .withColumn("table_ref", explode(col("potential_table_refs"))) \
    .withColumn("table_ref_clean", 
        when(col("table_ref").contains("."), col("table_ref"))
         .otherwise(concat(lit("???.???."), col("table_ref")))  # mark unqualified
    ) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "table_ref_clean",
        size("potential_table_refs").alias("table_count"),
        "used_catalog",
        "used_schema"
    )

# === Step 4: Show results (deduplicate per statement if you want cleaner view) ===
display(
    df_exploded
    .orderBy(col("query_start_time").desc())
    .limit(200)
)

# Optional: Most accessed tables (approximate)
df_exploded.groupBy("table_ref_clean") \
    .agg(
        count("*").alias("access_count"),
        min("query_start_time").alias("first_seen"),
        max("query_start_time").alias("last_seen")
    ) \
    .orderBy(col("access_count").desc()) \
    .display()



--------------------

table_name = "your_catalog.your_schema.query_history_longterm"

# Get current columns (case-insensitive)
desc_df = spark.sql(f"DESCRIBE TABLE {table_name}")
existing_cols = {row["col_name"].lower(): row["col_name"] for row in desc_df.collect()}

columns_to_add = [
    ("ingestion_ts", "TIMESTAMP"),
    ("email_processed_flag", "BOOLEAN")
]

added = []

for col_name, col_type in columns_to_add:
    if col_name.lower() not in existing_cols:
        spark.sql(f"""
            ALTER TABLE {table_name}
            ADD COLUMNS ({col_name} {col_type})
        """)
        added.append(col_name)

if added:
    print(f"Added columns: {', '.join(added)}")
else:
    print("All required columns already exist — no changes made")





INSERT INTO governance.access.allowed_actors VALUES
('finance_readonly', 'USER', 'jane.doe@company.com', 'jane.doe@company.com', true, current_timestamp(), NULL, 'security-team', 'INC12345', NULL, current_timestamp(), current_user(), current_timestamp(), current_user()),
('finance_readonly', 'SPN',  '00000000-0000-0000-0000-000000000000', 'spn-finance-bi', true, current_timestamp(), NULL, 'security-team', 'INC12346', NULL, current_timestamp(), current_user(), current_timestamp(), current_user());



CREATE TABLE IF NOT EXISTS governance.access.allowed_actors (
  policy_name        STRING,
  actor_type         STRING,        -- 'USER' | 'SPN' | 'UAMI'
  actor_id           STRING,        -- stable ID if you have it (recommended)
  actor_name         STRING,        -- email or display name (optional but useful)
  active             BOOLEAN,
  effective_from     TIMESTAMP,
  effective_to       TIMESTAMP,
  approved_by        STRING,
  ticket             STRING,
  notes              STRING,
  created_at         TIMESTAMP,
  created_by         STRING,
  updated_at         TIMESTAMP,
  updated_by         STRING
)
USING DELTA;




SELECT
  q.statement_id,
  q.executed_by,
  q.start_time,
  t.catalog_name,
  t.schema_name,
  t.table_name,
  t.access_type   -- READ / WRITE
FROM system.query.history q
JOIN system.query.table_lineage t
  ON q.statement_id = t.statement_id
WHERE q.statement_type = 'SELECT';



statement_id
start_time
end_time
executed_by <--------------
executed_by_user_id
client_application
warehouse_name
execution_status
duration_ms
query_text

rows_read
rows_written





ALTER TABLE your_catalog.your_schema.query_history_longterm
ADD COLUMNS (
  ingestion_ts TIMESTAMP,
  email_processed_flag BOOLEAN
);




-- In your ingestion task (e.g. scheduled notebook %sql or pure SQL job)
INSERT INTO your_catalog.your_schema.query_history_longterm
SELECT 
  *, 
  current_timestamp() AS ingestion_ts,
  FALSE AS email_processed_flag
FROM system.query.history
WHERE start_time > (SELECT COALESCE(MAX(ingestion_ts), TIMESTAMP '1900-01-01') FROM your_catalog.your_schema.query_history_longterm)





from pyspark.sql.functions import col
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
delta_target = DeltaTable.forName(spark, target_table)

# Find unprocessed records
unprocessed = (
    spark.read.table(target_table)
    .filter((col("email_processed_flag") == False) | col("email_processed_flag").isNull())
    # Optional: add filters, e.g. .filter("execution_status = 'FAILED'") or time window
)

if unprocessed.count() > 0:
    # Your email logic here (example placeholder)
    # Loop or batch-send emails based on unprocessed rows
    # For each row: send email about failed/cancelled query, using statement_text, executed_by, error_message, etc.
    # Example: print for demo
    unprocessed.select("statement_id", "executed_by_user_name", "statement_text", "error_message").show(truncate=False)
    
    # Simulate sending (replace with real email code)
    print("Sending emails for above records...")

    # Update flag for processed rows (safe: only those we just read)
    # Option A: Simple update on the whole set (if all succeeded)
    delta_target.update(
        condition = (col("email_processed_flag") == False) | col("email_processed_flag").isNull(),
        set = {"email_processed_flag": lit(True)}
    )

    # Option B: If some emails fail, update only successful ones (more complex; track per-row)





from pyspark.sql.functions import col, current_timestamp, lit
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
source_table = "system.query.history"

# Get last ingested timestamp (from ingestion_ts now, more reliable than start_time alone)
max_ts_df = spark.sql(f"SELECT COALESCE(MAX(ingestion_ts), '1900-01-01') AS max_ts FROM {target_table}")
max_ts = max_ts_df.collect()[0]["max_ts"]

# Fetch new data with buffer
new_data = (
    spark.read.table(source_table)
    .filter(col("start_time") > max_ts)  # still use start_time to avoid re-fetching everything
    .withColumn("ingestion_ts", current_timestamp())
    .withColumn("email_processed_flag", lit(False))  # default unprocessed
)

if new_data.count() > 0:
    # Append (fastest, since statement_id + workspace_id should be unique)
    new_data.write.format("delta").mode("append").saveAsTable(target_table)

    # Or safer MERGE if you see duplicates occasionally
    # delta_target = DeltaTable.forName(spark, target_table)
    # delta_target.alias("target").merge(
    #     new_data.alias("source"),
    #     "target.statement_id = source.statement_id AND target.workspace_id = source.workspace_id"
    # ).whenNotMatchedInsertAll().execute()






--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import json

# === Retrieve credentials from Databricks secrets ===
client_id     = dbutils.secrets.get(scope="my-graph-scope", key="client-id")
tenant_id     = dbutils.secrets.get(scope="my-graph-scope", key="tenant-id")
client_secret = dbutils.secrets.get(scope="my-graph-scope", key="client-secret")

# === 1. Get access token (client credentials flow) ===
token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"

payload = {
    "client_id": client_id,
    "scope": "https://graph.microsoft.com/.default",
    "client_secret": client_secret,
    "grant_type": "client_credentials"
}

response = requests.post(token_url, data=payload)
response.raise_for_status()
access_token = response.json()["access_token"]
print("✅ Token acquired")

# === 2. Send email ===
user_principal_name = "sender@yourdomain.com"   # ← The mailbox you want to send FROM
graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/sendMail"

email_body = {
    "message": {
        "subject": "Test Email from Databricks",
        "body": {
            "contentType": "HTML",
            "content": "<h1>Hello from Databricks!</h1><p>This email was sent via Microsoft Graph API.</p>"
        },
        "toRecipients": [
            {
                "emailAddress": {
                    "address": "recipient@yourdomain.com"
                }
            }
        ],
        # Optional: ccRecipients, bccRecipients
        # Optional: attachments (base64 encoded)
    },
    "saveToSentItems": True
}

headers = {
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json"
}

response = requests.post(graph_url, headers=headers, json=email_body)

if response.status_code == 202:
    print("✅ Email accepted for delivery")
else:
    print(f"❌ Error: {response.status_code}")
    print(response.text)








# ─── Most recommended solution ───

client_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"   # ← the ID you see in executed_as

query = f"""
Resources
| where type == 'microsoft.managedidentity/userassignedidentities'
| where properties.clientId == '{client_id}'
| project 
    name,
    resourceGroup,
    subscriptionId,
    principalId,
    clientId,
    id,
    location
"""

from azure.identity import DefaultAzureCredential
from azure.mgmt.resourcegraph import ResourceGraphClient

credential = DefaultAzureCredential()

# If you're using a cluster with UAMI attached → it will be picked up automatically
# Otherwise you can explicitly pass client_id of UAMI you want to use for auth
# credential = DefaultAzureCredential(managed_identity_client_id="your-uami-client-id")

rg_client = ResourceGraphClient(credential)

result = rg_client.resources(
    resources = {
        "query": query,
        "subscriptions": []   # ← empty = search in all accessible subscriptions
    }
)

if result.data:
    for row in result.data:
        print(f"UAMI Name        : {row['name']}")
        print(f"Resource Group   : {row['resourceGroup']}")
        print(f"Client ID        : {row['clientId']}")
        print(f"Object ID        : {row['principalId']}")
        print("-" * 60)
else:
    print("No UAMI found with this client id")







from pyspark.sql.functions import regexp_replace, col

df_clean = df.withColumn(
    "sql_stmt_clean",
    regexp_replace(
        regexp_replace(
            regexp_replace(col("sql_stmt"), r"[\n\r]+", " "),    # newlines → single space
            r"\t+", " "),                                        # tabs → space
        r" +", " "                                               # multiple spaces → single space
    )
)

# Optional: trim + collapse whitespace even more aggressively
# .withColumn("sql_stmt_clean", trim(regexp_replace(col("sql_stmt_clean"), r"\s+", " ")))

display(df_clean.select("id", "sql_stmt_clean", "other_col").limit(20))






# Databricks notebook source
# COMMAND ----------

# ────────────────────────────────────────────────
#          CONFIGURATION - CHANGE THESE
# ────────────────────────────────────────────────

CATALOG_NAME      = "main"                     # or your catalog name
SCHEMA_NAME       = "sales"
TABLE_NAME        = "orders"

DATE_COLUMN       = "order_date"               # ← important: your timestamp/date column name
EMAIL_RECIPIENTS  = ["manager@company.com", "teamlead@company.com"]
EMAIL_SUBJECT     = "Daily Orders Summary – " + dbutils.widgets.text("today", "")

YOUR_SENDER_NAME  = "Databricks Alerts <alerts@yourcompany.com>"

# If you want to use SMTP instead of Databricks email → fill these
USE_SMTP          = False
SMTP_HOST         = "smtp.office365.com"
SMTP_PORT         = 587
SMTP_USER         = "your.email@company.com"
SMTP_PASSWORD     = dbutils.secrets.get("email-secrets", "smtp-password")  # ← recommended

# ────────────────────────────────────────────────
#               1. READ TODAY'S DATA
# ────────────────────────────────────────────────

from pyspark.sql.functions import col, current_date, to_date, lit
from datetime import date

today = date.today()
print(f"Processing date: {today}")

table_full_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

df_today = (
    spark.table(table_full_name)
    .where(to_date(col(DATE_COLUMN)) == lit(today))
    # .where(col(DATE_COLUMN).cast("date") == current_date())   # alternative
)

count = df_today.count()
print(f"Found {count:,} records for today")

if count == 0:
    dbutils.notebook.exit("No records found for today → stopping")

# COMMAND ----------

# ────────────────────────────────────────────────
#               2. BUILD NICE HTML TABLE
# ────────────────────────────────────────────────

# Optional: take a sample or aggregate — here we show raw records (limit to avoid huge emails)
display_limit = 500
pdf = df_today.limit(display_limit).toPandas()

# Convert to HTML with basic but clean styling
html_table = pdf.to_html(
    index=False,
    border=0,
    classes="table table-striped table-hover",
    justify="left"
)

html_content = f"""\
<html>
<head>
  <style>
    body {{ font-family: -apple-system, BlinkMacOSystemFont, 'Segoe UI', Roboto, sans-serif; color: #333; }}
    .header {{ background: #1e3a8a; color: white; padding: 16px; border-radius: 6px 6px 0 0; }}
    .content {{ padding: 20px; background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0 0 6px 6px; }}
    table {{ width: 100%; border-collapse: collapse; }}
    th, td {{ padding: 10px 12px; text-align: left; border-bottom: 1px solid #e5e7eb; }}
    th {{ background: #f3f4f6; font-weight: 600; }}
    .footer {{ margin-top: 24px; font-size: 0.9em; color: #6b7280; }}
  </style>
</head>
<body>
  <div class="header">
    <h2>Daily Orders Report – {today}</h2>
    <p>Found <strong>{count:,}</strong> new orders today</p>
  </div>
  
  <div class="content">
    {html_table}
    
    <div class="footer">
      {'Showing first ' + str(display_limit) + ' rows' if count > display_limit else 'All records shown'}<br>
      Full dataset available in Databricks → {table_full_name}
    </div>
  </div>
</body>
</html>
"""

# COMMAND ----------

# ────────────────────────────────────────────────
#               3. SEND EMAIL
# ────────────────────────────────────────────────

if not USE_SMTP:
    # === Preferred way in Databricks (no credentials needed) ===
    dbutils.notebook.run("/Shared/send_html_email", 120, {
        "subject": f"{EMAIL_SUBJECT} {today}",
        "html_body": html_content,
        "to": ",".join(EMAIL_RECIPIENTS),
        "from_name": YOUR_SENDER_NAME
    })
    print("Email sent using Databricks internal email service")

else:
    # === Classic SMTP way (Outlook 365 / Gmail etc) ===
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"{EMAIL_SUBJECT} {today}"
    msg["From"]    = YOUR_SENDER_NAME
    msg["To"]      = ", ".join(EMAIL_RECIPIENTS)

    msg.attach(MIMEText(html_content, "html"))

    try:
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        print("Email sent successfully via SMTP")
    except Exception as e:
        print("SMTP failed:", str(e))

# COMMAND ----------

# Optional: show preview in notebook
displayHTML(html_content[:32000])  # avoid notebook crash with very large tables
