from pyspark.sql.functions import col, lit
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
source_table = "system.query.history"

# Get last ingested timestamp (or use a separate control/metadata table for robustness)
max_ts_df = spark.sql(f"SELECT COALESCE(MAX(start_time), '1900-01-01') AS max_ts FROM {target_table}")
max_ts = max_ts_df.collect()[0]["max_ts"]

# Fetch new data (add small buffer window e.g., -1 hour to catch late-arriving records)
new_data = spark.read.table(source_table).filter(col("start_time") > max_ts)

if new_data.count() > 0:
    # Option A: Simple append (fastest if statement_id is reliably unique and no re-runs)
    new_data.write.format("delta").mode("append").saveAsTable(target_table)

    # Option B: Safer MERGE (handles duplicates, late data, re-runs; use if worried about exact-once)
    delta_target = DeltaTable.forName(spark, target_table)
    delta_target.alias("target").merge(
        new_data.alias("source"),
        "target.statement_id = source.statement_id AND target.workspace_id = source.workspace_id"  # or add start_time
    ).whenNotMatchedInsertAll().execute()
