# Databricks notebook (PySpark) â€” incremental archive of system.query.history into Unity Catalog
# Pattern: scheduled batch + overlap window + MERGE (upsert) for long-term retention

from pyspark.sql import functions as F

# ----------------------------
# 0) CONFIG (edit these)
# ----------------------------
TARGET_CATALOG = "project_catalog"
TARGET_SCHEMA  = "observability"
TARGET_TABLE   = "query_history"          # final Delta table
META_TABLE     = "query_history_watermark" # small table to track last ingested time

# How far back to re-read each run (handles late arriving updates / retries)
OVERLAP_DAYS = 2

# If you want to only archive terminal states, set to True
ONLY_TERMINAL = False

# ----------------------------
# 1) Ensure schema + metadata table exist
# ----------------------------
spark.sql(f"CREATE CATALOG IF NOT EXISTS {TARGET_CATALOG}")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}")

# Metadata table stores one row with the watermark timestamp (max start_time ingested)
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}.{META_TABLE} (
  pipeline_name STRING,
  last_start_time TIMESTAMP,
  updated_at TIMESTAMP
)
USING DELTA
""")

# Insert initial row if absent
spark.sql(f"""
INSERT INTO {TARGET_CATALOG}.{TARGET_SCHEMA}.{META_TABLE}
SELECT 'system.query.history', CAST('1900-01-01' AS TIMESTAMP), current_timestamp()
WHERE NOT EXISTS (
  SELECT 1 FROM {TARGET_CATALOG}.{TARGET_SCHEMA}.{META_TABLE}
  WHERE pipeline_name = 'system.query.history'
)
""")

# ----------------------------
# 2) Read current watermark
# ----------------------------
wm = (
    spark.table(f"{TARGET_CATALOG}.{TARGET_SCHEMA}.{META_TABLE}")
    .where(F.col("pipeline_name") == F.lit("system.query.history"))
    .select("last_start_time")
    .collect()[0][0]
)

start_from = spark.sql(f"SELECT timestamp('{wm}') as ts").collect()[0]["ts"]
# apply overlap
start_from_overlap = spark.sql(
    f"SELECT timestamp('{wm}') - INTERVAL {OVERLAP_DAYS} DAYS AS ts"
).collect()[0]["ts"]

print(f"Watermark last_start_time = {start_from}")
print(f"Reading from (with overlap) = {start_from_overlap}")

# ----------------------------
# 3) Build source dataframe (incremental slice)
# ----------------------------
src = (
    spark.table("system.query.history")
    .where(F.col("start_time") >= F.lit(start_from_overlap))
)

if ONLY_TERMINAL:
    src = src.where(F.col("execution_status").isin(["FINISHED", "FAILED", "CANCELED"]))

# Optional: de-dup within the batch by statement_id, keeping the latest update
# (Some fields may update; if there is an update timestamp column in your environment, prefer it.
# If not, we can approximate with end_time then start_time.)
order_cols = [F.col("end_time").desc_nulls_last(), F.col("start_time").desc_nulls_last()]
src_dedup = (
    src.withColumn("_rn", F.row_number().over(
        __import__("pyspark").sql.Window.partitionBy("statement_id").orderBy(*order_cols)
    ))
    .where(F.col("_rn") == 1)
    .drop("_rn")
)

# ----------------------------
# 4) Create target table if it doesn't exist (clone schema from source)
# ----------------------------
target_fqn = f"{TARGET_CATALOG}.{TARGET_SCHEMA}.{TARGET_TABLE}"

# If table doesn't exist, create it with the same schema
if not spark.catalog.tableExists(target_fqn):
    (src_dedup.limit(0)
        .write
        .format("delta")
        .mode("overwrite")
        .saveAsTable(target_fqn)
    )
    # Optional: partitioning for long-term retention table (choose what fits your workloads)
    # If you'd like partitioning by date(start_time), uncomment below and recreate table accordingly.
    # Partitioning changes require recreating the table, so decide early.

# Enable schema evolution for MERGE (handles new columns added by Databricks)
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# ----------------------------
# 5) MERGE (upsert) into target
# ----------------------------
src_view = "src_query_history_incremental"
src_dedup.createOrReplaceTempView(src_view)

# Use statement_id as the primary key. If you are in multi-workspace/account context,
# consider (statement_id, workspace_id) if workspace_id exists in your table.
merge_sql = f"""
MERGE INTO {target_fqn} AS t
USING {src_view} AS s
ON t.statement_id = s.statement_id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
"""
spark.sql(merge_sql)

# ----------------------------
# 6) Advance watermark (max start_time ingested this run)
# ----------------------------
max_start = spark.table(target_fqn).agg(F.max("start_time").alias("mx")).collect()[0]["mx"]

spark.sql(f"""
UPDATE {TARGET_CATALOG}.{TARGET_SCHEMA}.{META_TABLE}
SET last_start_time = TIMESTAMP('{max_start}'),
    updated_at = current_timestamp()
WHERE pipeline_name = 'system.query.history'
""")

print(f"Updated watermark to {max_start}")

# ----------------------------
# 7) (Optional) Optimize / Z-Order / Liquid clustering
# ----------------------------
# If you're on a Databricks runtime that supports it and you query this table a lot,
# consider periodic OPTIMIZE (scheduled separately, e.g., daily/weekly).
# Examples (pick one approach):
#
# spark.sql(f"OPTIMIZE {target_fqn} ZORDER BY (start_time, executed_by_user_id)")
#
# Or if you use Liquid clustering (recommended newer pattern), configure via table properties
# outside the notebook or in a one-time migration script.
