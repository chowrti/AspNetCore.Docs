CREATE TABLE IF NOT EXISTS governance.access.allowed_actors (
  policy_name        STRING,
  actor_type         STRING,        -- 'USER' | 'SPN' | 'UAMI'
  actor_id           STRING,        -- stable ID if you have it (recommended)
  actor_name         STRING,        -- email or display name (optional but useful)
  active             BOOLEAN,
  effective_from     TIMESTAMP,
  effective_to       TIMESTAMP,
  approved_by        STRING,
  ticket             STRING,
  notes              STRING,
  created_at         TIMESTAMP,
  created_by         STRING,
  updated_at         TIMESTAMP,
  updated_by         STRING
)
USING DELTA;




SELECT
  q.statement_id,
  q.executed_by,
  q.start_time,
  t.catalog_name,
  t.schema_name,
  t.table_name,
  t.access_type   -- READ / WRITE
FROM system.query.history q
JOIN system.query.table_lineage t
  ON q.statement_id = t.statement_id
WHERE q.statement_type = 'SELECT';



statement_id
start_time
end_time
executed_by <--------------
executed_by_user_id
client_application
warehouse_name
execution_status
duration_ms
query_text

rows_read
rows_written





ALTER TABLE your_catalog.your_schema.query_history_longterm
ADD COLUMNS (
  ingestion_ts TIMESTAMP,
  email_processed_flag BOOLEAN
);




-- In your ingestion task (e.g. scheduled notebook %sql or pure SQL job)
INSERT INTO your_catalog.your_schema.query_history_longterm
SELECT 
  *, 
  current_timestamp() AS ingestion_ts,
  FALSE AS email_processed_flag
FROM system.query.history
WHERE start_time > (SELECT COALESCE(MAX(ingestion_ts), TIMESTAMP '1900-01-01') FROM your_catalog.your_schema.query_history_longterm)





from pyspark.sql.functions import col
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
delta_target = DeltaTable.forName(spark, target_table)

# Find unprocessed records
unprocessed = (
    spark.read.table(target_table)
    .filter((col("email_processed_flag") == False) | col("email_processed_flag").isNull())
    # Optional: add filters, e.g. .filter("execution_status = 'FAILED'") or time window
)

if unprocessed.count() > 0:
    # Your email logic here (example placeholder)
    # Loop or batch-send emails based on unprocessed rows
    # For each row: send email about failed/cancelled query, using statement_text, executed_by, error_message, etc.
    # Example: print for demo
    unprocessed.select("statement_id", "executed_by_user_name", "statement_text", "error_message").show(truncate=False)
    
    # Simulate sending (replace with real email code)
    print("Sending emails for above records...")

    # Update flag for processed rows (safe: only those we just read)
    # Option A: Simple update on the whole set (if all succeeded)
    delta_target.update(
        condition = (col("email_processed_flag") == False) | col("email_processed_flag").isNull(),
        set = {"email_processed_flag": lit(True)}
    )

    # Option B: If some emails fail, update only successful ones (more complex; track per-row)





from pyspark.sql.functions import col, current_timestamp, lit
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
source_table = "system.query.history"

# Get last ingested timestamp (from ingestion_ts now, more reliable than start_time alone)
max_ts_df = spark.sql(f"SELECT COALESCE(MAX(ingestion_ts), '1900-01-01') AS max_ts FROM {target_table}")
max_ts = max_ts_df.collect()[0]["max_ts"]

# Fetch new data with buffer
new_data = (
    spark.read.table(source_table)
    .filter(col("start_time") > max_ts)  # still use start_time to avoid re-fetching everything
    .withColumn("ingestion_ts", current_timestamp())
    .withColumn("email_processed_flag", lit(False))  # default unprocessed
)

if new_data.count() > 0:
    # Append (fastest, since statement_id + workspace_id should be unique)
    new_data.write.format("delta").mode("append").saveAsTable(target_table)

    # Or safer MERGE if you see duplicates occasionally
    # delta_target = DeltaTable.forName(spark, target_table)
    # delta_target.alias("target").merge(
    #     new_data.alias("source"),
    #     "target.statement_id = source.statement_id AND target.workspace_id = source.workspace_id"
    # ).whenNotMatchedInsertAll().execute()






--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import json

# === Retrieve credentials from Databricks secrets ===
client_id     = dbutils.secrets.get(scope="my-graph-scope", key="client-id")
tenant_id     = dbutils.secrets.get(scope="my-graph-scope", key="tenant-id")
client_secret = dbutils.secrets.get(scope="my-graph-scope", key="client-secret")

# === 1. Get access token (client credentials flow) ===
token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"

payload = {
    "client_id": client_id,
    "scope": "https://graph.microsoft.com/.default",
    "client_secret": client_secret,
    "grant_type": "client_credentials"
}

response = requests.post(token_url, data=payload)
response.raise_for_status()
access_token = response.json()["access_token"]
print("✅ Token acquired")

# === 2. Send email ===
user_principal_name = "sender@yourdomain.com"   # ← The mailbox you want to send FROM
graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/sendMail"

email_body = {
    "message": {
        "subject": "Test Email from Databricks",
        "body": {
            "contentType": "HTML",
            "content": "<h1>Hello from Databricks!</h1><p>This email was sent via Microsoft Graph API.</p>"
        },
        "toRecipients": [
            {
                "emailAddress": {
                    "address": "recipient@yourdomain.com"
                }
            }
        ],
        # Optional: ccRecipients, bccRecipients
        # Optional: attachments (base64 encoded)
    },
    "saveToSentItems": True
}

headers = {
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json"
}

response = requests.post(graph_url, headers=headers, json=email_body)

if response.status_code == 202:
    print("✅ Email accepted for delivery")
else:
    print(f"❌ Error: {response.status_code}")
    print(response.text)








# ─── Most recommended solution ───

client_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"   # ← the ID you see in executed_as

query = f"""
Resources
| where type == 'microsoft.managedidentity/userassignedidentities'
| where properties.clientId == '{client_id}'
| project 
    name,
    resourceGroup,
    subscriptionId,
    principalId,
    clientId,
    id,
    location
"""

from azure.identity import DefaultAzureCredential
from azure.mgmt.resourcegraph import ResourceGraphClient

credential = DefaultAzureCredential()

# If you're using a cluster with UAMI attached → it will be picked up automatically
# Otherwise you can explicitly pass client_id of UAMI you want to use for auth
# credential = DefaultAzureCredential(managed_identity_client_id="your-uami-client-id")

rg_client = ResourceGraphClient(credential)

result = rg_client.resources(
    resources = {
        "query": query,
        "subscriptions": []   # ← empty = search in all accessible subscriptions
    }
)

if result.data:
    for row in result.data:
        print(f"UAMI Name        : {row['name']}")
        print(f"Resource Group   : {row['resourceGroup']}")
        print(f"Client ID        : {row['clientId']}")
        print(f"Object ID        : {row['principalId']}")
        print("-" * 60)
else:
    print("No UAMI found with this client id")







from pyspark.sql.functions import regexp_replace, col

df_clean = df.withColumn(
    "sql_stmt_clean",
    regexp_replace(
        regexp_replace(
            regexp_replace(col("sql_stmt"), r"[\n\r]+", " "),    # newlines → single space
            r"\t+", " "),                                        # tabs → space
        r" +", " "                                               # multiple spaces → single space
    )
)

# Optional: trim + collapse whitespace even more aggressively
# .withColumn("sql_stmt_clean", trim(regexp_replace(col("sql_stmt_clean"), r"\s+", " ")))

display(df_clean.select("id", "sql_stmt_clean", "other_col").limit(20))






# Databricks notebook source
# COMMAND ----------

# ────────────────────────────────────────────────
#          CONFIGURATION - CHANGE THESE
# ────────────────────────────────────────────────

CATALOG_NAME      = "main"                     # or your catalog name
SCHEMA_NAME       = "sales"
TABLE_NAME        = "orders"

DATE_COLUMN       = "order_date"               # ← important: your timestamp/date column name
EMAIL_RECIPIENTS  = ["manager@company.com", "teamlead@company.com"]
EMAIL_SUBJECT     = "Daily Orders Summary – " + dbutils.widgets.text("today", "")

YOUR_SENDER_NAME  = "Databricks Alerts <alerts@yourcompany.com>"

# If you want to use SMTP instead of Databricks email → fill these
USE_SMTP          = False
SMTP_HOST         = "smtp.office365.com"
SMTP_PORT         = 587
SMTP_USER         = "your.email@company.com"
SMTP_PASSWORD     = dbutils.secrets.get("email-secrets", "smtp-password")  # ← recommended

# ────────────────────────────────────────────────
#               1. READ TODAY'S DATA
# ────────────────────────────────────────────────

from pyspark.sql.functions import col, current_date, to_date, lit
from datetime import date

today = date.today()
print(f"Processing date: {today}")

table_full_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

df_today = (
    spark.table(table_full_name)
    .where(to_date(col(DATE_COLUMN)) == lit(today))
    # .where(col(DATE_COLUMN).cast("date") == current_date())   # alternative
)

count = df_today.count()
print(f"Found {count:,} records for today")

if count == 0:
    dbutils.notebook.exit("No records found for today → stopping")

# COMMAND ----------

# ────────────────────────────────────────────────
#               2. BUILD NICE HTML TABLE
# ────────────────────────────────────────────────

# Optional: take a sample or aggregate — here we show raw records (limit to avoid huge emails)
display_limit = 500
pdf = df_today.limit(display_limit).toPandas()

# Convert to HTML with basic but clean styling
html_table = pdf.to_html(
    index=False,
    border=0,
    classes="table table-striped table-hover",
    justify="left"
)

html_content = f"""\
<html>
<head>
  <style>
    body {{ font-family: -apple-system, BlinkMacOSystemFont, 'Segoe UI', Roboto, sans-serif; color: #333; }}
    .header {{ background: #1e3a8a; color: white; padding: 16px; border-radius: 6px 6px 0 0; }}
    .content {{ padding: 20px; background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0 0 6px 6px; }}
    table {{ width: 100%; border-collapse: collapse; }}
    th, td {{ padding: 10px 12px; text-align: left; border-bottom: 1px solid #e5e7eb; }}
    th {{ background: #f3f4f6; font-weight: 600; }}
    .footer {{ margin-top: 24px; font-size: 0.9em; color: #6b7280; }}
  </style>
</head>
<body>
  <div class="header">
    <h2>Daily Orders Report – {today}</h2>
    <p>Found <strong>{count:,}</strong> new orders today</p>
  </div>
  
  <div class="content">
    {html_table}
    
    <div class="footer">
      {'Showing first ' + str(display_limit) + ' rows' if count > display_limit else 'All records shown'}<br>
      Full dataset available in Databricks → {table_full_name}
    </div>
  </div>
</body>
</html>
"""

# COMMAND ----------

# ────────────────────────────────────────────────
#               3. SEND EMAIL
# ────────────────────────────────────────────────

if not USE_SMTP:
    # === Preferred way in Databricks (no credentials needed) ===
    dbutils.notebook.run("/Shared/send_html_email", 120, {
        "subject": f"{EMAIL_SUBJECT} {today}",
        "html_body": html_content,
        "to": ",".join(EMAIL_RECIPIENTS),
        "from_name": YOUR_SENDER_NAME
    })
    print("Email sent using Databricks internal email service")

else:
    # === Classic SMTP way (Outlook 365 / Gmail etc) ===
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"{EMAIL_SUBJECT} {today}"
    msg["From"]    = YOUR_SENDER_NAME
    msg["To"]      = ", ".join(EMAIL_RECIPIENTS)

    msg.attach(MIMEText(html_content, "html"))

    try:
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        print("Email sent successfully via SMTP")
    except Exception as e:
        print("SMTP failed:", str(e))

# COMMAND ----------

# Optional: show preview in notebook
displayHTML(html_content[:32000])  # avoid notebook crash with very large tables
