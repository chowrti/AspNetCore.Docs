Here are suggested **bullet points** and slide structure ideas for your **PowerPoint presentation/demo** on this security controls project. These are concise, professional, and focused on explaining the value, how it works, and key benefits during a demo.

### Slide Title Ideas & Bullet Points

**Slide 1: Project Overview – Inadvertent Access Monitoring for Sensitive Data**

- Objective: Detect and alert on unauthorized or inadvertent SQL queries accessing sensitive tables in the data lake (Unity Catalog)
- Risk addressed: Accidental exposure of PII, financial, PHI, or other regulated data via ad-hoc queries, misconfigured tools, or human error
- Scope: Near real-time alerting for SELECT activity on flagged sensitive objects
- Key benefit: Proactive governance + audit readiness without blocking legitimate access

**Slide 2: Solution Architecture – High-Level Flow**

- Unity Catalog + system.query.history table → Core source of truth for all query activity
- Sensitive Tables Lookup → Curated table listing protected objects (catalog.schema.table + metadata)
- Allowed Actors Whitelist → Table of approved accounts/groups (no alerts triggered)
- Monitoring Job/Workflow → Scheduled notebook or Delta Live Tables pipeline
  - Queries system.query.history periodically (e.g., every 5–15 min)
  - Filters for SELECT statements + sensitive table references (text match or lineage join)
  - Excludes allowed actors
  - Triggers alert if match found
- Alert Delivery → Email (Graph API / Logic App), Teams/Slack webhook, or PagerDuty

**Slide 3: Core Data Sources**

- system.query.history (Unity Catalog system table)
  - Captures every query: statement_text, executed_by_user_name, start_time, statement_type, compute details
  - Retention: 365 days (regional)
  - Near real-time: Updated throughout the day (not instant, but sufficient for near-real-time alerting)
- Sensitive Tables Lookup (custom table)
  - Columns: full_table_name, sensitivity_level (PII/Financial/etc.), owner_team, is_active
- Allowed Actors Whitelist (custom table)
  - Columns: user_name / service_principal / group_name, justification, approved_by, expiry_date

**Slide 4: Detection Logic – How We Identify Inadvertent Access**

- Filter criteria:
  - statement_type = 'SELECT' (focus on reads)
  - statement_text contains sensitive full_table_name (case-insensitive LIKE / contains)
  - executed_by_user_name NOT IN allowed_actors
- Optional enhancement: Join with system.access.table_lineage
  - More accurate than text search (resolves views, CTEs, joins)
  - Captures actual accessed tables (action_type = 'READ')
- Exclusion logic: Whitelist prevents alert fatigue for approved users (e.g., data stewards, auditors)

**Slide 5: Alerting Mechanism & Near Real-Time Approach**

- Scheduled monitoring job (Databricks Workflows / Jobs)
  - Frequency: 5–15 minutes (balance latency vs cost)
  - Query window: Last run time → current_timestamp()
- Alert content example:
  - Query executed by: user@company.com
  - Time: 2026-02-03 14:30 UTC
  - Sensitive table(s): main.finance.customer_payments
  - Full query snippet
  - Link to query history / lineage
- Delivery: Email to security team + optional escalation (high-sensitivity tables)

**Slide 6: Demo Highlights (Live Walkthrough Suggestions)**

- Show sensitive_tables & allowed_actors tables (SELECT sample rows)
- Run sample monitoring query live (e.g., last hour's matches)
- Simulate inadvertent access: Run SELECT on sensitive table from non-whitelisted user
- Show alert received (email / Teams message)
- Demonstrate exclusion: Query from allowed actor → no alert
- Optional: Show lineage join for accuracy vs text search

**Slide 7: Benefits & Outcomes**

- Early detection of risky access before data exfiltration or compliance violation
- Reduced manual audit effort – automated, auditable monitoring
- Low false positives via whitelist + optional lineage
- Governance alignment: Supports SOX, GDPR, HIPAA, etc. requirements
- Scalable: Leverages native Unity Catalog system tables (no external agents)

**Slide 8: Next Steps & Roadmap**

- Phase 1: MVP with text-based detection + email alerts
- Phase 2: Integrate table_lineage for higher fidelity
- Phase 3: Add column-level checks (if data_classification enabled)
- Phase 4: Auto-remediation (e.g., temporary revoke) or dashboard in Databricks SQL
- Ongoing: Maintain sensitive/allowed lists via governance process

### Additional Suggestions for Your Deck
- Use simple icons: Eye for monitoring, Shield for security, Bell for alerts, Table for lookups
- Include 1–2 screenshots: Query results, sample alert email, lineage graph from Catalog Explorer
- Add a timeline: "Query runs → system table updated → monitoring job detects → alert sent (within ~10–20 min)"
- Emphasize: "No performance impact on users – reads system tables only"
- Call out limitations: Not truly real-time (system tables update throughout the day), text search has potential false positives/negatives → lineage as future improvement

These bullets keep slides clean (3–6 per slide max) while telling a clear story: problem → solution → how it works → value → demo proof.

If you want more detail on any slide (e.g., sample query code for the monitoring job or alert template text), just let me know! Good luck with the presentation.

------------


from pyspark.sql.functions import col, lower

# Get list of sensitive full names (or table names) - broadcast small lookup
sensitive_list = [row.full_table_name for row in sensitive_df.select("full_table_name").distinct().collect()]
# or just table_name if you don't have full qualified: sensitive_list = [row.table_name ...]

# For better performance on small lookup → use broadcast hint or collect to array
sensitive_broadcast = sensitive_df.select("full_table_name").distinct()

# Filter df1: keep rows where statement_text does NOT contain ANY sensitive full name
df_filtered = df1.join(
    sensitive_broadcast.hint("broadcast"),
    lower(col("statement_text")).contains(lower(col("full_table_name"))),
    "left_anti"   # ← key: left anti join = rows from left with NO match on condition
)

df_filtered.show(truncate=False)



---------------------


-- 1. Create the lookup table (one row per sensitive table)
CREATE TABLE IF NOT EXISTS governance.sensitive_monitoring.sensitive_tables (
  catalog_name     STRING NOT NULL COMMENT 'e.g., main, finance, hr',
  schema_name      STRING NOT NULL COMMENT 'e.g., prod, staging',
  table_name       STRING NOT NULL COMMENT 'table name without catalog/schema',
  full_table_name  STRING GENERATED ALWAYS AS (CONCAT(catalog_name, '.', schema_name, '.', table_name)) 
    STORED AS STRING COMMENT 'For easy joins / lookups',
  sensitivity_level STRING COMMENT 'e.g., PII, PHI, Financial, Confidential, Restricted',
  owner_team       STRING COMMENT 'Owning team for follow-up',
  last_reviewed_date DATE COMMENT 'When last governance review occurred',
  notes            STRING COMMENT 'Additional context or risk notes',
  is_active        BOOLEAN DEFAULT TRUE COMMENT 'Set to FALSE if no longer sensitive'
)
USING DELTA
LOCATION 'abfss://your-container@yourstorage.dfs.core.windows.net/sensitive_monitoring/'   -- optional, for managed location
COMMENT 'Lookup of tables considered sensitive - used to scan query.history';

-- 2. Add some example data (replace with your real sensitive tables)
INSERT INTO governance.sensitive_monitoring.sensitive_tables
  (catalog_name, schema_name, table_name, sensitivity_level, owner_team, last_reviewed_date, notes)
VALUES
  ('main',      'finance',   'customer_payments',    'Financial',  'Finance Team',   '2026-01-15', 'Contains credit card tokens & amounts'),
  ('hr',        'prod',      'employee_personal',     'PII',        'HR',             '2026-01-10', 'SSN, DOB, address'),
  ('sales',     'staging',   'leads_with_emails',     'PII',        'Marketing',      '2025-12-20', 'Email + phone'),
  ('compliance','audit',     'audit_logs_raw',        'Confidential','Compliance',    '2026-02-01', 'Internal audit trail')
;

-- 3. (Optional) Add primary key / unique constraint for data quality
ALTER TABLE governance.sensitive_monitoring.sensitive_tables 
  ADD CONSTRAINT pk_sensitive_tables PRIMARY KEY (full_table_name);



-------------

EXEC master.dbo.sp_addlinkedserver 
    @server         = N'DATABRICKS_UAMI',              -- Your chosen linked server name
    @srvproduct     = N'Databricks',
    @provider       = N'MSDASQL',                      -- OLE DB Provider for ODBC
    @datasrc        = NULL,                            -- DSN-less, so blank
    @provstr        = N'Driver={Simba Spark ODBC Driver};'
                    + N'Host=adb-1234567890123456.7.azuredatabricks.net;'  -- ← your hostname
                    + N'Port=443;'
                    + N'HTTPPath=/sql/1.0/warehouses/your-warehouse-id;'   -- ← your HTTP Path
                    + N'SSL=1;'
                    + N'ThriftTransport=2;'
                    + N'SparkServerType=3;'            -- 3 = SQL Warehouse
                    + N'AuthMech=11;'                  -- OAuth 2.0
                    + N'Auth_Flow=3;'                  -- Azure Managed Identity flow
                    + N'Auth_Client_ID=123e4567-e89b-12d3-a456-426614174000;'  -- ← YOUR UAMI Client ID here
                    + N'Azure_workspace_resource_id=/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/your-rg/providers/Microsoft.Databricks/workspaces/your-ws-name;'
                    + N'Catalog=main;';                -- Optional: default catalog

-- No sp_addlinkedsrvlogin needed for managed identity flow (no UID/PWD)
-- The driver acquires token automatically via IMDS


-------------

from pyspark.sql.functions import col, explode, regexp_extract_all, lower, regexp_replace
import re

# Step 1: Fetch recent queries (adjust filters)
df_queries = spark.sql("""
  SELECT 
    statement_id,
    executed_by,
    start_time,
    statement_type,
    statement_text
  FROM system.query.history
  WHERE start_time >= current_timestamp() - INTERVAL 7 DAY
    AND statement_type IN ('SELECT', 'INSERT', 'UPDATE', 'MERGE', 'DELETE')  -- focus on DML/DQL
    AND statement_text IS NOT NULL
  ORDER BY start_time DESC
  LIMIT 500   -- for testing; remove in prod
""")

# Step 2: Clean text a bit (remove comments, extra whitespace)
df_clean = df_queries.withColumn(
    "clean_text",
    regexp_replace(lower(col("statement_text")), r'--.*?$|/\*.*?\*/|\s+', ' ')
)

# Step 3: Regex to find potential table references
# Pattern explanation:
#   \\b([a-z0-9_]+\\.)?([a-z0-9_]+\\.)?([a-z0-9_]+)\\b
#   → optional catalog. + optional schema. + table
#   Look for FROM/JOIN/INTO clauses roughly
pattern = r'(?:from|join|into|update|merge|describe|describe history)\s+([a-z0-9_]+(?:\.[a-z0-9_]+){0,2})\b'

df_extracted = df_clean.withColumn(
    "potential_refs",
    regexp_extract_all(col("clean_text"), lit(pattern), lit(1))
).withColumn(
    "table_refs", explode(col("potential_refs"))
).filter(col("table_refs").isNotNull())

# Step 4: Split into catalog/schema/table
df_parsed = df_extracted.withColumn(
    "parts", 
    split(regexp_replace(col("table_refs"), r'[^a-z0-9_\.]', ''), r'\.')
).withColumn(
    "num_parts", size(col("parts"))
).withColumn(
    "catalog",
    when(col("num_parts") == 3, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "schema",
    when(col("num_parts") == 3, element_at(col("parts"), 2))
      .when(col("num_parts") == 2, element_at(col("parts"), 1))
      .otherwise(lit(None))
).withColumn(
    "table",
    when(col("num_parts") == 3, element_at(col("parts"), 3))
      .when(col("num_parts") == 2, element_at(col("parts"), 2))
      .when(col("num_parts") == 1, element_at(col("parts"), 1))
      .otherwise(lit(None))
)

# Display results
display(df_parsed.select(
    "statement_id", "executed_by", "start_time", 
    "statement_text", "catalog", "schema", "table"
).orderBy(col("start_time").desc()))



-----------------

EXEC master.dbo.sp_addlinkedserver 
    @server         = N'DATABRICKS_DSNLESS',          -- Friendly name for the linked server (use in queries)
    @srvproduct     = N'Databricks SQL',
    @provider       = N'MSDASQL',                     -- Microsoft OLE DB Provider for ODBC Drivers
    @datasrc        = NULL,                           -- Leave blank for DSN-less
    @provstr        = N'Driver={Simba Spark ODBC Driver};'
                     + N'Host=adb-1234567890123456.7.azuredatabricks.net;'  -- ← your hostname
                     + N'Port=443;'
                     + N'HTTPPath=/sql/1.0/warehouses/abc123def456789;'     -- ← your HTTP Path
                     + N'SSL=1;'
                     + N'ThriftTransport=2;'           -- HTTP transport
                     + N'SparkServerType=3;'           -- 3 = SQL Warehouse (use 1 for interactive cluster)
                     + N'AuthMech=3;'                  -- 3 = User Name + Password (token)
                     + N'UID=token;'
                     + N'PWD=your_personal_access_token_here;'  -- ← your PAT (keep secure!)
                     + N'Catalog=main;';               -- Optional: sets default catalog (e.g. main, hive_metastore, or custom Unity Catalog)

-- Add security/login mapping (token-based, no Windows auth usually)
EXEC master.dbo.sp_addlinkedsrvlogin 
    @rmtsrvname = N'DATABRICKS_DSNLESS',
    @useself    = N'False',
    @locallogin = NULL,                       -- Applies to all local logins
    @rmtuser    = N'token',
    @rmtpassword = N'your_personal_access_token_here';  -- Same PAT as above


---------------

As a **Principal Architect** specializing in Azure data platforms, here's a clear, production-oriented guide to creating a **simple yet robust Azure Data Factory (ADF) pipeline** that reads multiple Parquet tables (folders/files) from **Azure Data Lake Storage Gen2 (ADLS Gen2)**.

This example assumes a common pattern:  
- Read Parquet files from a few source folders in ADLS Gen2 (Bronze/raw zone).  
- Optionally copy/transform them to another location (e.g., Silver zone, another container, or database).  
For simplicity, we'll copy the Parquet data as-is (or merge small files) to a target ADLS folder — a frequent starting point before adding transformations.

### Prerequisites (Everything You Need Before Starting)

| #  | Prerequisite                              | Details / Why Required                                                                 | Owner / Action Needed                          |
|----|-------------------------------------------|----------------------------------------------------------------------------------------|------------------------------------------------|
| 1  | Active Azure Subscription                 | ADF and ADLS Gen2 require an Azure subscription                                       | Subscription Owner / Create free trial if needed |
| 2  | Azure Data Factory instance (v2)          | The orchestration service. Use ADF in a region close to your storage for performance | Create via Azure Portal → Search "Data factories" |
| 3  | Azure Data Lake Storage Gen2 account      | Hierarchical namespace **must** be enabled (it's the default for new Gen2 accounts)   | Create Storage Account → Enable "Hierarchical namespace" |
| 4  | At least one container in ADLS Gen2       | e.g., `bronze`, `silver`                                                               | Create container(s) in the storage account     |
| 5  | Parquet files already present             | Folders like `/bronze/sales/year=2025/month=01/` with `.parquet` or `.snappy.parquet` files | Upload test files or use existing data         |
| 6  | Permissions (RBAC)                        | - **Contributor** or **Owner** on ADF & Storage resource groups<br>- Storage Blob Data Contributor / Reader on the ADLS account (or via ABAC) | Assign roles via Azure Portal → Access control (IAM) |
| 7  | Managed Identity or Access Key (recommended: Managed Identity) | ADF uses this to authenticate to ADLS Gen2 without secrets                             | Enable system-assigned managed identity on ADF |
| 8  | Browser + Azure Portal / ADF Studio       | Authoring UI (modern ADF authoring experience)                                         | Access via https://adf.azure.com               |
| 9  | (Optional but recommended)                | Azure Key Vault (for secrets if using account key instead of MI)                       | —                                              |
|10  | Basic familiarity with ADF concepts       | Linked Services, Datasets, Pipelines, Activities                                       | —                                              |

**Security note (2026 best practice)**:  
Always prefer **system-assigned managed identity** + **Storage Blob Data Contributor** role scoped to the container(s). Avoid account keys in production.

### Step-by-Step: Create a Simple ADF Pipeline to Read Parquet Files

#### Phase 1 – Prepare Linked Service & Datasets

1. **Open ADF Studio**  
   Azure Portal → Your Data Factory → Launch studio (or go directly to adf.azure.com)

2. **Create Linked Service to ADLS Gen2 (Source & Target can share one)**  
   - Manage tab → Linked services → New  
   - Search & select **Azure Data Lake Storage Gen2**  
   - Name: `LS_ADLS_Main` (or similar)  
   - Connect via **Managed Identity** (recommended) → Test connection  
     - If using account key: select Account key → paste from Storage Account → Keys  
   - Save

3. **Create Source Dataset (Parquet – parameterized for flexibility)**  
   - Author tab → Datasets → New dataset  
   - Azure Data Lake Storage Gen2 → Continue  
   - Format: **Parquet** → Continue  
   - Linked service: `LS_ADLS_Main`  
   - Name: `DS_Source_Parquet`  
   - **Parameters** tab: Add two parameters  
     - `Container` (String, default: `bronze`)  
     - `FolderPath` (String, default: `sales`)  
   - Connection tab → File path:  
     - Container = `@dataset().Container`  
     - Directory = `@dataset().FolderPath`  
   - Save

4. **Create Target Dataset (similar – Parquet sink)**  
   - New dataset → Azure Data Lake Storage Gen2 → Parquet  
   - Name: `DS_Target_Parquet`  
   - Linked service: same `LS_ADLS_Main`  
   - Parameters: `TargetContainer` (default: `silver`), `TargetFolder` (default: `processed/sales`)  
   - File path: use parameters like source  
   - (Optional) Compression: Snappy or none

#### Phase 2 – Build the Pipeline

5. **Create New Pipeline**  
   - Author tab → Pipelines → New pipeline  
   - Name: `PL_Read_Parquet_Tables`

6. **Add Copy Activity (one per table – or use ForEach for multiple)**  
   Simple version (3 tables = 3 copy activities):

   - Drag **Copy data** activity from Activities → Move & Transform  
   - Name: `Copy_Sales_Parquet`  
   - Source tab:  
     - Source dataset: `DS_Source_Parquet`  
     - Dataset properties:  
       - Container = `bronze`  
       - FolderPath = `sales/year=*/month=*`   (wildcard to read all partitions)  
   - Sink tab:  
     - Sink dataset: `DS_Target_Parquet`  
     - Dataset properties:  
       - TargetContainer = `silver`  
       - TargetFolder = `sales`  
     - Copy behavior: **PreserveHierarchy** (or FlattenHierarchy / MergeFiles if small files)  
   - Settings tab:  
     - Enable **Fault tolerance** if desired (skip incompatible rows)  
     - Logging level: Basic or Information

   Repeat for other tables (e.g., `customers`, `products`) by copying the activity and changing folder paths.

7. **(Better scalability) Use ForEach + Get Metadata for multiple tables**  
   - Add **Get Metadata** activity → get child folders from `/bronze`  
   - Add **ForEach** activity → items = `@activity('Get Metadata').output.childItems`  
   - Inside ForEach: Add Copy activity with dynamic content:  
     - FolderPath = `@item().name`

#### Phase 3 – Test & Publish

8. **Debug the pipeline**  
   - Click **Debug** → monitor progress  
   - Check output JSON, row counts, errors  
   - Validate files appeared in target folder

9. **Publish All**  
   - Click Publish → publish changes to factory

10. **Trigger (manual or schedule)**  
    - Add trigger: Manual / Schedule / Tumbling window  
    - For production: use **Event trigger** (blob created) or **Schedule trigger**

### Quick Architecture Summary (Medallion Style)

- **Bronze** (`bronze` container): raw Parquet as landed  
- **Silver** (`silver` container): cleaned / lightly transformed Parquet (this pipeline copies here)  
- Later add: Mapping Data Flow or Databricks for transformations

This pattern is used in production at scale — simple to maintain, cost-effective (serverless), and easy to extend with parameters, metadata-driven tables, or incremental logic.

If you need:
- Incremental load (watermark / last modified)
- Merge small files
- Load to Synapse / Fabric Lakehouse / SQL DB
- Error handling & alerting

...let me know — I can extend the design accordingly.



--------------------

from pyspark.sql.functions import (
    col, lower, regexp_extract_all, explode, 
    array_distinct, when, size, current_timestamp
)

# === Step 1: Read the system.query.history table ===
# Filter to recent queries (adjust interval as needed)
df = spark.table("system.query.history") \
    .where(col("query_start_time") >= current_timestamp() - expr("INTERVAL 7 DAYS")) \
    .where(col("statement_type") == "QUERY") \
    .where(col("statement_text").isNotNull()) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "statement_type",
        "executed_as_user_name"
    )

# === Step 2: Normalize and extract table-like patterns with regex ===
# Common patterns we want to catch:
#   catalog.schema.table
#   schema.table
#   table   (unqualified – harder to resolve without context)

df_with_extracted = df.withColumn(
    "normalized_sql", 
    lower(col("statement_text"))
).withColumn(
    # Extract anything that looks like a 1–3 part identifier after FROM/JOIN/INSERT/UPDATE/MERGE/etc.
    "potential_table_refs",
    regexp_extract_all(
        col("normalized_sql"),
        r"(?:from|join|insert\s+into|update|merge\s+into|create\s+or\s+replace\s+view|create\s+view|drop\s+table|alter\s+table)\s+([a-z0-9_]+\.[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+\.[a-z0-9_]+|[a-z0-9_]+)\b",
        1
    )
).withColumn(
    # Also try to catch USE CATALOG / USE statements
    "used_catalog",
    regexp_extract_all(col("normalized_sql"), r"use\s+catalog\s+([a-z0-9_]+)", 1)
).withColumn(
    "used_schema",
    regexp_extract_all(col("normalized_sql"), r"use\s+([a-z0-9_]+)", 1)
)

# === Step 3: Explode the array of extracted tables for easier analysis ===
df_exploded = df_with_extracted \
    .withColumn("table_ref", explode(col("potential_table_refs"))) \
    .withColumn("table_ref_clean", 
        when(col("table_ref").contains("."), col("table_ref"))
         .otherwise(concat(lit("???.???."), col("table_ref")))  # mark unqualified
    ) \
    .select(
        "statement_id",
        "query_start_time",
        "executed_by_user_name",
        "statement_text",
        "table_ref_clean",
        size("potential_table_refs").alias("table_count"),
        "used_catalog",
        "used_schema"
    )

# === Step 4: Show results (deduplicate per statement if you want cleaner view) ===
display(
    df_exploded
    .orderBy(col("query_start_time").desc())
    .limit(200)
)

# Optional: Most accessed tables (approximate)
df_exploded.groupBy("table_ref_clean") \
    .agg(
        count("*").alias("access_count"),
        min("query_start_time").alias("first_seen"),
        max("query_start_time").alias("last_seen")
    ) \
    .orderBy(col("access_count").desc()) \
    .display()



--------------------

table_name = "your_catalog.your_schema.query_history_longterm"

# Get current columns (case-insensitive)
desc_df = spark.sql(f"DESCRIBE TABLE {table_name}")
existing_cols = {row["col_name"].lower(): row["col_name"] for row in desc_df.collect()}

columns_to_add = [
    ("ingestion_ts", "TIMESTAMP"),
    ("email_processed_flag", "BOOLEAN")
]

added = []

for col_name, col_type in columns_to_add:
    if col_name.lower() not in existing_cols:
        spark.sql(f"""
            ALTER TABLE {table_name}
            ADD COLUMNS ({col_name} {col_type})
        """)
        added.append(col_name)

if added:
    print(f"Added columns: {', '.join(added)}")
else:
    print("All required columns already exist — no changes made")





INSERT INTO governance.access.allowed_actors VALUES
('finance_readonly', 'USER', 'jane.doe@company.com', 'jane.doe@company.com', true, current_timestamp(), NULL, 'security-team', 'INC12345', NULL, current_timestamp(), current_user(), current_timestamp(), current_user()),
('finance_readonly', 'SPN',  '00000000-0000-0000-0000-000000000000', 'spn-finance-bi', true, current_timestamp(), NULL, 'security-team', 'INC12346', NULL, current_timestamp(), current_user(), current_timestamp(), current_user());



CREATE TABLE IF NOT EXISTS governance.access.allowed_actors (
  policy_name        STRING,
  actor_type         STRING,        -- 'USER' | 'SPN' | 'UAMI'
  actor_id           STRING,        -- stable ID if you have it (recommended)
  actor_name         STRING,        -- email or display name (optional but useful)
  active             BOOLEAN,
  effective_from     TIMESTAMP,
  effective_to       TIMESTAMP,
  approved_by        STRING,
  ticket             STRING,
  notes              STRING,
  created_at         TIMESTAMP,
  created_by         STRING,
  updated_at         TIMESTAMP,
  updated_by         STRING
)
USING DELTA;




SELECT
  q.statement_id,
  q.executed_by,
  q.start_time,
  t.catalog_name,
  t.schema_name,
  t.table_name,
  t.access_type   -- READ / WRITE
FROM system.query.history q
JOIN system.query.table_lineage t
  ON q.statement_id = t.statement_id
WHERE q.statement_type = 'SELECT';



statement_id
start_time
end_time
executed_by <--------------
executed_by_user_id
client_application
warehouse_name
execution_status
duration_ms
query_text

rows_read
rows_written





ALTER TABLE your_catalog.your_schema.query_history_longterm
ADD COLUMNS (
  ingestion_ts TIMESTAMP,
  email_processed_flag BOOLEAN
);




-- In your ingestion task (e.g. scheduled notebook %sql or pure SQL job)
INSERT INTO your_catalog.your_schema.query_history_longterm
SELECT 
  *, 
  current_timestamp() AS ingestion_ts,
  FALSE AS email_processed_flag
FROM system.query.history
WHERE start_time > (SELECT COALESCE(MAX(ingestion_ts), TIMESTAMP '1900-01-01') FROM your_catalog.your_schema.query_history_longterm)





from pyspark.sql.functions import col
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
delta_target = DeltaTable.forName(spark, target_table)

# Find unprocessed records
unprocessed = (
    spark.read.table(target_table)
    .filter((col("email_processed_flag") == False) | col("email_processed_flag").isNull())
    # Optional: add filters, e.g. .filter("execution_status = 'FAILED'") or time window
)

if unprocessed.count() > 0:
    # Your email logic here (example placeholder)
    # Loop or batch-send emails based on unprocessed rows
    # For each row: send email about failed/cancelled query, using statement_text, executed_by, error_message, etc.
    # Example: print for demo
    unprocessed.select("statement_id", "executed_by_user_name", "statement_text", "error_message").show(truncate=False)
    
    # Simulate sending (replace with real email code)
    print("Sending emails for above records...")

    # Update flag for processed rows (safe: only those we just read)
    # Option A: Simple update on the whole set (if all succeeded)
    delta_target.update(
        condition = (col("email_processed_flag") == False) | col("email_processed_flag").isNull(),
        set = {"email_processed_flag": lit(True)}
    )

    # Option B: If some emails fail, update only successful ones (more complex; track per-row)





from pyspark.sql.functions import col, current_timestamp, lit
from delta.tables import DeltaTable

target_table = "your_catalog.your_schema.query_history_longterm"
source_table = "system.query.history"

# Get last ingested timestamp (from ingestion_ts now, more reliable than start_time alone)
max_ts_df = spark.sql(f"SELECT COALESCE(MAX(ingestion_ts), '1900-01-01') AS max_ts FROM {target_table}")
max_ts = max_ts_df.collect()[0]["max_ts"]

# Fetch new data with buffer
new_data = (
    spark.read.table(source_table)
    .filter(col("start_time") > max_ts)  # still use start_time to avoid re-fetching everything
    .withColumn("ingestion_ts", current_timestamp())
    .withColumn("email_processed_flag", lit(False))  # default unprocessed
)

if new_data.count() > 0:
    # Append (fastest, since statement_id + workspace_id should be unique)
    new_data.write.format("delta").mode("append").saveAsTable(target_table)

    # Or safer MERGE if you see duplicates occasionally
    # delta_target = DeltaTable.forName(spark, target_table)
    # delta_target.alias("target").merge(
    #     new_data.alias("source"),
    #     "target.statement_id = source.statement_id AND target.workspace_id = source.workspace_id"
    # ).whenNotMatchedInsertAll().execute()






--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import json

# === Retrieve credentials from Databricks secrets ===
client_id     = dbutils.secrets.get(scope="my-graph-scope", key="client-id")
tenant_id     = dbutils.secrets.get(scope="my-graph-scope", key="tenant-id")
client_secret = dbutils.secrets.get(scope="my-graph-scope", key="client-secret")

# === 1. Get access token (client credentials flow) ===
token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"

payload = {
    "client_id": client_id,
    "scope": "https://graph.microsoft.com/.default",
    "client_secret": client_secret,
    "grant_type": "client_credentials"
}

response = requests.post(token_url, data=payload)
response.raise_for_status()
access_token = response.json()["access_token"]
print("✅ Token acquired")

# === 2. Send email ===
user_principal_name = "sender@yourdomain.com"   # ← The mailbox you want to send FROM
graph_url = f"https://graph.microsoft.com/v1.0/users/{user_principal_name}/sendMail"

email_body = {
    "message": {
        "subject": "Test Email from Databricks",
        "body": {
            "contentType": "HTML",
            "content": "<h1>Hello from Databricks!</h1><p>This email was sent via Microsoft Graph API.</p>"
        },
        "toRecipients": [
            {
                "emailAddress": {
                    "address": "recipient@yourdomain.com"
                }
            }
        ],
        # Optional: ccRecipients, bccRecipients
        # Optional: attachments (base64 encoded)
    },
    "saveToSentItems": True
}

headers = {
    "Authorization": f"Bearer {access_token}",
    "Content-Type": "application/json"
}

response = requests.post(graph_url, headers=headers, json=email_body)

if response.status_code == 202:
    print("✅ Email accepted for delivery")
else:
    print(f"❌ Error: {response.status_code}")
    print(response.text)








# ─── Most recommended solution ───

client_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"   # ← the ID you see in executed_as

query = f"""
Resources
| where type == 'microsoft.managedidentity/userassignedidentities'
| where properties.clientId == '{client_id}'
| project 
    name,
    resourceGroup,
    subscriptionId,
    principalId,
    clientId,
    id,
    location
"""

from azure.identity import DefaultAzureCredential
from azure.mgmt.resourcegraph import ResourceGraphClient

credential = DefaultAzureCredential()

# If you're using a cluster with UAMI attached → it will be picked up automatically
# Otherwise you can explicitly pass client_id of UAMI you want to use for auth
# credential = DefaultAzureCredential(managed_identity_client_id="your-uami-client-id")

rg_client = ResourceGraphClient(credential)

result = rg_client.resources(
    resources = {
        "query": query,
        "subscriptions": []   # ← empty = search in all accessible subscriptions
    }
)

if result.data:
    for row in result.data:
        print(f"UAMI Name        : {row['name']}")
        print(f"Resource Group   : {row['resourceGroup']}")
        print(f"Client ID        : {row['clientId']}")
        print(f"Object ID        : {row['principalId']}")
        print("-" * 60)
else:
    print("No UAMI found with this client id")







from pyspark.sql.functions import regexp_replace, col

df_clean = df.withColumn(
    "sql_stmt_clean",
    regexp_replace(
        regexp_replace(
            regexp_replace(col("sql_stmt"), r"[\n\r]+", " "),    # newlines → single space
            r"\t+", " "),                                        # tabs → space
        r" +", " "                                               # multiple spaces → single space
    )
)

# Optional: trim + collapse whitespace even more aggressively
# .withColumn("sql_stmt_clean", trim(regexp_replace(col("sql_stmt_clean"), r"\s+", " ")))

display(df_clean.select("id", "sql_stmt_clean", "other_col").limit(20))






# Databricks notebook source
# COMMAND ----------

# ────────────────────────────────────────────────
#          CONFIGURATION - CHANGE THESE
# ────────────────────────────────────────────────

CATALOG_NAME      = "main"                     # or your catalog name
SCHEMA_NAME       = "sales"
TABLE_NAME        = "orders"

DATE_COLUMN       = "order_date"               # ← important: your timestamp/date column name
EMAIL_RECIPIENTS  = ["manager@company.com", "teamlead@company.com"]
EMAIL_SUBJECT     = "Daily Orders Summary – " + dbutils.widgets.text("today", "")

YOUR_SENDER_NAME  = "Databricks Alerts <alerts@yourcompany.com>"

# If you want to use SMTP instead of Databricks email → fill these
USE_SMTP          = False
SMTP_HOST         = "smtp.office365.com"
SMTP_PORT         = 587
SMTP_USER         = "your.email@company.com"
SMTP_PASSWORD     = dbutils.secrets.get("email-secrets", "smtp-password")  # ← recommended

# ────────────────────────────────────────────────
#               1. READ TODAY'S DATA
# ────────────────────────────────────────────────

from pyspark.sql.functions import col, current_date, to_date, lit
from datetime import date

today = date.today()
print(f"Processing date: {today}")

table_full_name = f"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}"

df_today = (
    spark.table(table_full_name)
    .where(to_date(col(DATE_COLUMN)) == lit(today))
    # .where(col(DATE_COLUMN).cast("date") == current_date())   # alternative
)

count = df_today.count()
print(f"Found {count:,} records for today")

if count == 0:
    dbutils.notebook.exit("No records found for today → stopping")

# COMMAND ----------

# ────────────────────────────────────────────────
#               2. BUILD NICE HTML TABLE
# ────────────────────────────────────────────────

# Optional: take a sample or aggregate — here we show raw records (limit to avoid huge emails)
display_limit = 500
pdf = df_today.limit(display_limit).toPandas()

# Convert to HTML with basic but clean styling
html_table = pdf.to_html(
    index=False,
    border=0,
    classes="table table-striped table-hover",
    justify="left"
)

html_content = f"""\
<html>
<head>
  <style>
    body {{ font-family: -apple-system, BlinkMacOSystemFont, 'Segoe UI', Roboto, sans-serif; color: #333; }}
    .header {{ background: #1e3a8a; color: white; padding: 16px; border-radius: 6px 6px 0 0; }}
    .content {{ padding: 20px; background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0 0 6px 6px; }}
    table {{ width: 100%; border-collapse: collapse; }}
    th, td {{ padding: 10px 12px; text-align: left; border-bottom: 1px solid #e5e7eb; }}
    th {{ background: #f3f4f6; font-weight: 600; }}
    .footer {{ margin-top: 24px; font-size: 0.9em; color: #6b7280; }}
  </style>
</head>
<body>
  <div class="header">
    <h2>Daily Orders Report – {today}</h2>
    <p>Found <strong>{count:,}</strong> new orders today</p>
  </div>
  
  <div class="content">
    {html_table}
    
    <div class="footer">
      {'Showing first ' + str(display_limit) + ' rows' if count > display_limit else 'All records shown'}<br>
      Full dataset available in Databricks → {table_full_name}
    </div>
  </div>
</body>
</html>
"""

# COMMAND ----------

# ────────────────────────────────────────────────
#               3. SEND EMAIL
# ────────────────────────────────────────────────

if not USE_SMTP:
    # === Preferred way in Databricks (no credentials needed) ===
    dbutils.notebook.run("/Shared/send_html_email", 120, {
        "subject": f"{EMAIL_SUBJECT} {today}",
        "html_body": html_content,
        "to": ",".join(EMAIL_RECIPIENTS),
        "from_name": YOUR_SENDER_NAME
    })
    print("Email sent using Databricks internal email service")

else:
    # === Classic SMTP way (Outlook 365 / Gmail etc) ===
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"{EMAIL_SUBJECT} {today}"
    msg["From"]    = YOUR_SENDER_NAME
    msg["To"]      = ", ".join(EMAIL_RECIPIENTS)

    msg.attach(MIMEText(html_content, "html"))

    try:
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)
        print("Email sent successfully via SMTP")
    except Exception as e:
        print("SMTP failed:", str(e))

# COMMAND ----------

# Optional: show preview in notebook
displayHTML(html_content[:32000])  # avoid notebook crash with very large tables
